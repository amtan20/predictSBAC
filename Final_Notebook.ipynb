{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "# Predicting SBAC\n",
    "by Amee Tan "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of Contents \n",
    "----\n",
    "1. Research Question\n",
    "2. Evaluation Metrics\n",
    "3. Load Data\n",
    "4. Search for best algorithms\n",
    "5. Tune hyperparameters for 3 candidate models\n",
    "6. Compare 3 candidate models and select final model\n",
    "7. Run final model on test set\n",
    "8. Examine feature importances for the final model\n",
    "9. Conclusion and next steps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Research Question \n",
    "----\n",
    "Can we use student demographic information and test scores throughout the year to predict whether or not a student will pass the Math SBAC at the end of the year? Which features are most important for making the prediction? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Metrics  \n",
    "----\n",
    "2 evaluation metrics are used in this notebook: precision & accuracy \n",
    "\n",
    "**Precision:** \n",
    "- Since the goal is to produce a model that reduces false positives, precision is the best way to measure the model's performance.\n",
    "- A false positive means the model predicts that a student will pass SBAC when in reality they fail. This is problematic because it means there are students who should have received extra support during the school year did not receive it because the model did not identify them as being on track to fail.\n",
    "\n",
    "**Accuracy:** \n",
    "- It is also important to know the overall accuracy of the model. Given that the target class had a 66/34 split, it is important to make sure the model outperforms the a priori probability (meaning the model needs to achieve an accuracy score of 0.66 or higher)\n",
    "- I debated whether to use accuracy or balanced accuracy. Balanced accuracy is better when there are class imbalances, but since the target had about a 66/34 split between the two classes, I decided this was not enough of a class imbalance to merit using balanced accuracy. Furthermore, balanced accuracy is defined as the average of recall obtained on each class. Recall is not as important for this case, so I stuck with accuracy score as my second metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from   sklearn.base            import BaseEstimator\n",
    "from   sklearn.compose         import *\n",
    "from   sklearn.ensemble        import RandomForestClassifier, ExtraTreesClassifier, IsolationForest, GradientBoostingClassifier\n",
    "from   sklearn.experimental    import enable_iterative_imputer\n",
    "from   sklearn.impute          import *\n",
    "from   sklearn.inspection      import permutation_importance\n",
    "from   sklearn.linear_model    import LogisticRegression, PassiveAggressiveClassifier, RidgeClassifier, SGDClassifier\n",
    "from   sklearn.metrics         import precision_score, classification_report, accuracy_score, confusion_matrix\n",
    "from   sklearn.model_selection import cross_validate, cross_val_score, KFold, RandomizedSearchCV, train_test_split\n",
    "from   sklearn.neighbors       import *\n",
    "from   sklearn.pipeline        import Pipeline\n",
    "from   sklearn.preprocessing   import *\n",
    "from   sklearn.svm             import SVC\n",
    "from   sklearn.tree            import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('https://github.com/amtan20/predictSBAC/raw/main/public_student_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y dataframes \n",
    "X = data.drop(columns=['Mathematics Achievement Level','ELA/Literacy Achievement Level'])\n",
    "y = data['Mathematics Achievement Level']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up column names \n",
    "\n",
    "X.rename(columns={\"Reading Fall '18 %ile\" : \"Reading Fall Percentile\", \n",
    "                  \"Reading Winter '18 %ile\" : \"Reading Winter Percentile\", \n",
    "                  \"Math Fall '18 %ile\" : \"Math Fall Percentile\", \n",
    "                  \"Winter '18 %ile\" : \"Math Winter Percentile\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target Engineering \n",
    "-----\n",
    "The data in the target column originally has 4 levels:\n",
    "- Standard Not Met (Fail)\n",
    "- Standard Nearly Met (Fail)\n",
    "- Standard Met (Pass)\n",
    "- Standard Exceeded (Pass)\n",
    "\n",
    "For this project, I was more interested in determining if a student would pass or fail because this binary classification is more important for teachers than the multi-class classification. The first two levels indicate that a student failed. The last two levels indicate that a student passed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change y into binary numeric target \n",
    "def create_binary_target(y):\n",
    "    return np.where(y=='Standard Not Met',0, np.where(y=='Standard Nearly Met', 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use FunctionTransformer to apply the function to the target \n",
    "transformer = FunctionTransformer(create_binary_target)\n",
    "y = transformer.transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Train, Validation, and Test Sets \n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=46)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('X_train.csv')\n",
    "pd.DataFrame(y_train).to_csv('y_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Pipeline \n",
    "----\n",
    "Impute missing values, one hot encode categorical features, standardize numeric features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_columns = ['IDEA Indicator', 'LEP Status', 'Economically Disadvantaged Status']\n",
    "\n",
    "categorical_columns = ['Grade _x', \n",
    "       'Reading \\nMet Winter Goal?', \n",
    "       'Math Met Winter Goal?', \n",
    "       'Race/Ethnicity', 'Language Code',\n",
    "       'English Language Proficiency Level', 'Migrant Status',\n",
    "       'Primary Disability Type']\n",
    "\n",
    "numeric_columns = ['Reading Fall Percentile',\n",
    "       'Reading Winter Percentile', 'Reading Fall Score to Winter Growth',\n",
    "       'Math Fall Percentile',\n",
    "       'Math Winter Percentile', 'Math Fall to Winter Growth',\n",
    "       'Math Met Winter Goal?', \n",
    "       'First Entry Date Into US School', 'LEP Entry Date', 'LEP Exit Date']\n",
    "\n",
    "\n",
    "boolean_pipe = Pipeline([('imputer', SimpleImputer(strategy='constant', fill_value='False')), \n",
    "                         ('ohe', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "categorical_pipe = Pipeline([('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "                             ('ohe', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "numeric_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                         ('imputer', SimpleImputer(strategy='median'))])\n",
    "\n",
    "preprocessing = ColumnTransformer([('boolean', boolean_pipe,  boolean_columns),\n",
    "                                   ('categorical', categorical_pipe, categorical_columns),\n",
    "                                   ('numeric',  numeric_pipe, numeric_columns)])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search for best algorithms using randomized search cross validation\n",
    "----\n",
    "RandomizedSearchCV will return a single best model. I want to have 3 candidate models so I will run this 3 times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create helper class \n",
    "class DummyEstimator(BaseEstimator):\n",
    "    \"Pass through class, methods are present but do nothing.\"\n",
    "    def fit(self): pass\n",
    "    def score(self): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessing',\n",
       "                 ColumnTransformer(transformers=[('boolean',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(fill_value='False',\n",
       "                                                                                 strategy='constant')),\n",
       "                                                                  ('ohe',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                  ['IDEA Indicator',\n",
       "                                                   'LEP Status',\n",
       "                                                   'Economically Disadvantaged '\n",
       "                                                   'Status']),\n",
       "                                                 ('categorical',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(fill_value='missing'...\n",
       "                                                  Pipeline(steps=[('scaler',\n",
       "                                                                   StandardScaler()),\n",
       "                                                                  ('imputer',\n",
       "                                                                   SimpleImputer(strategy='median'))]),\n",
       "                                                  ['Reading Fall Percentile',\n",
       "                                                   'Reading Winter Percentile',\n",
       "                                                   'Reading Fall Score to '\n",
       "                                                   'Winter Growth',\n",
       "                                                   'Math Fall Percentile',\n",
       "                                                   'Math Winter Percentile',\n",
       "                                                   'Math Fall to Winter Growth',\n",
       "                                                   'Math Met Winter Goal?',\n",
       "                                                   'First Entry Date Into US '\n",
       "                                                   'School',\n",
       "                                                   'LEP Entry Date',\n",
       "                                                   'LEP Exit Date'])])),\n",
       "                ('mod', ExtraTreesClassifier())])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1st iteration \n",
    "\n",
    "pipe = Pipeline([('preprocessing', preprocessing),\n",
    "                 ('mod', DummyEstimator())])\n",
    "\n",
    "models = [{'mod' : [ExtraTreesClassifier()]},\n",
    "          {'mod' : [GradientBoostingClassifier()]},\n",
    "          {'mod' : [KNeighborsClassifier()]},\n",
    "          {'mod' : [LogisticRegression()]},\n",
    "          {'mod' : [PassiveAggressiveClassifier()]},\n",
    "          {'mod' : [RandomForestClassifier()]},\n",
    "          {'mod' : [RidgeClassifier()]},\n",
    "          {'mod' : [SGDClassifier()]},\n",
    "          {'mod' : [SVC()]}\n",
    "          ]\n",
    "\n",
    "clf_rand = RandomizedSearchCV(estimator=pipe, \n",
    "                              param_distributions=models, \n",
    "                              n_iter=5,\n",
    "                              cv=5,\n",
    "                              scoring='precision',\n",
    "                              n_jobs=-1)\n",
    "best_model = clf_rand.fit(X_train, y_train) \n",
    "best_model.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessing',\n",
       "                 ColumnTransformer(transformers=[('boolean',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(fill_value='False',\n",
       "                                                                                 strategy='constant')),\n",
       "                                                                  ('ohe',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                  ['LEP Status',\n",
       "                                                   'Economically Disadvantaged '\n",
       "                                                   'Status']),\n",
       "                                                 ('categorical',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(fill_value='missing',\n",
       "                                                                                 strategy='constant')),\n",
       "                                                                  ('ohe',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                  ['Math Met Winter Goal?',\n",
       "                                                   'Race/Ethnicity']),\n",
       "                                                 ('numeric',\n",
       "                                                  Pipeline(steps=[('scaler',\n",
       "                                                                   StandardScaler()),\n",
       "                                                                  ('imputer',\n",
       "                                                                   SimpleImputer(strategy='median'))]),\n",
       "                                                  ['Reading Winter Percentile',\n",
       "                                                   'Math Winter '\n",
       "                                                   'Percentile'])])),\n",
       "                ('mod', SGDClassifier())])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2nd iteration \n",
    "\n",
    "clf_rand = RandomizedSearchCV(estimator=pipe, \n",
    "                              param_distributions=models, \n",
    "                              n_iter=5,\n",
    "                              cv=5,\n",
    "                              scoring='precision',\n",
    "                              n_jobs=-1)\n",
    "best_model = clf_rand.fit(X_train, y_train) \n",
    "best_model.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessing',\n",
       "                 ColumnTransformer(transformers=[('boolean',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(fill_value='False',\n",
       "                                                                                 strategy='constant')),\n",
       "                                                                  ('ohe',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                  ['LEP Status',\n",
       "                                                   'Economically Disadvantaged '\n",
       "                                                   'Status']),\n",
       "                                                 ('categorical',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(fill_value='missing',\n",
       "                                                                                 strategy='constant')),\n",
       "                                                                  ('ohe',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                  ['Math Met Winter Goal?',\n",
       "                                                   'Race/Ethnicity']),\n",
       "                                                 ('numeric',\n",
       "                                                  Pipeline(steps=[('scaler',\n",
       "                                                                   StandardScaler()),\n",
       "                                                                  ('imputer',\n",
       "                                                                   SimpleImputer(strategy='median'))]),\n",
       "                                                  ['Reading Winter Percentile',\n",
       "                                                   'Math Winter '\n",
       "                                                   'Percentile'])])),\n",
       "                ('mod', LogisticRegression())])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3rd iteration \n",
    "\n",
    "clf_rand = RandomizedSearchCV(estimator=pipe, \n",
    "                              param_distributions=models, \n",
    "                              n_iter=5,\n",
    "                              cv=5,\n",
    "                              scoring='precision',\n",
    "                              n_jobs=-1)\n",
    "best_model = clf_rand.fit(X_train, y_train) \n",
    "best_model.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper Parameter Tuning and KFold Cross Validation for each candidate model\n",
    "----\n",
    "Now that I have my 3 candidate models, I will:\n",
    "1. Do a randomized search with cross validation to find the best hyperparameters for each model. \n",
    "2. Run a KFold Cross Validation to get the precision and accuracy scores for the tuned models. I will get 10 precision and accuracy scores from the KFold Cross Validation and will take the mean. Taking the average of 10 scores will give me a more stable estimate of the performance of the model on the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    1.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('preprocessing',\n",
       "   ColumnTransformer(transformers=[('boolean',\n",
       "                                    Pipeline(steps=[('imputer',\n",
       "                                                     SimpleImputer(fill_value='False',\n",
       "                                                                   strategy='constant')),\n",
       "                                                    ('ohe',\n",
       "                                                     OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                    ['IDEA Indicator', 'LEP Status',\n",
       "                                     'Economically Disadvantaged Status']),\n",
       "                                   ('categorical',\n",
       "                                    Pipeline(steps=[('imputer',\n",
       "                                                     SimpleImputer(fill_value='missing',\n",
       "                                                                   strategy='constant')),\n",
       "                                                    ('ohe',\n",
       "                                                     OneHo...\n",
       "                                     'Primary Disability Type']),\n",
       "                                   ('numeric',\n",
       "                                    Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                                    ('imputer',\n",
       "                                                     SimpleImputer(strategy='median'))]),\n",
       "                                    ['Reading Fall Percentile',\n",
       "                                     'Reading Winter Percentile',\n",
       "                                     'Reading Fall Score to Winter Growth',\n",
       "                                     'Math Fall Percentile',\n",
       "                                     'Math Winter Percentile',\n",
       "                                     'Math Fall to Winter Growth',\n",
       "                                     'Math Met Winter Goal?',\n",
       "                                     'First Entry Date Into US School',\n",
       "                                     'LEP Entry Date', 'LEP Exit Date'])])),\n",
       "  ('classifier',\n",
       "   LogisticRegression(C=10000.0, class_weight='balanced', fit_intercept=False,\n",
       "                      max_iter=500, multi_class='multinomial', penalty='l1',\n",
       "                      solver='saga'))],\n",
       " 'verbose': False,\n",
       " 'preprocessing': ColumnTransformer(transformers=[('boolean',\n",
       "                                  Pipeline(steps=[('imputer',\n",
       "                                                   SimpleImputer(fill_value='False',\n",
       "                                                                 strategy='constant')),\n",
       "                                                  ('ohe',\n",
       "                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                  ['IDEA Indicator', 'LEP Status',\n",
       "                                   'Economically Disadvantaged Status']),\n",
       "                                 ('categorical',\n",
       "                                  Pipeline(steps=[('imputer',\n",
       "                                                   SimpleImputer(fill_value='missing',\n",
       "                                                                 strategy='constant')),\n",
       "                                                  ('ohe',\n",
       "                                                   OneHo...\n",
       "                                   'Primary Disability Type']),\n",
       "                                 ('numeric',\n",
       "                                  Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                                  ('imputer',\n",
       "                                                   SimpleImputer(strategy='median'))]),\n",
       "                                  ['Reading Fall Percentile',\n",
       "                                   'Reading Winter Percentile',\n",
       "                                   'Reading Fall Score to Winter Growth',\n",
       "                                   'Math Fall Percentile',\n",
       "                                   'Math Winter Percentile',\n",
       "                                   'Math Fall to Winter Growth',\n",
       "                                   'Math Met Winter Goal?',\n",
       "                                   'First Entry Date Into US School',\n",
       "                                   'LEP Entry Date', 'LEP Exit Date'])]),\n",
       " 'classifier': LogisticRegression(C=10000.0, class_weight='balanced', fit_intercept=False,\n",
       "                    max_iter=500, multi_class='multinomial', penalty='l1',\n",
       "                    solver='saga'),\n",
       " 'preprocessing__n_jobs': None,\n",
       " 'preprocessing__remainder': 'drop',\n",
       " 'preprocessing__sparse_threshold': 0.3,\n",
       " 'preprocessing__transformer_weights': None,\n",
       " 'preprocessing__transformers': [('boolean',\n",
       "   Pipeline(steps=[('imputer',\n",
       "                    SimpleImputer(fill_value='False', strategy='constant')),\n",
       "                   ('ohe', OneHotEncoder(handle_unknown='ignore'))]),\n",
       "   ['IDEA Indicator', 'LEP Status', 'Economically Disadvantaged Status']),\n",
       "  ('categorical',\n",
       "   Pipeline(steps=[('imputer',\n",
       "                    SimpleImputer(fill_value='missing', strategy='constant')),\n",
       "                   ('ohe', OneHotEncoder(handle_unknown='ignore'))]),\n",
       "   ['Grade _x',\n",
       "    'Reading \\nMet Winter Goal?',\n",
       "    'Math Met Winter Goal?',\n",
       "    'Race/Ethnicity',\n",
       "    'Language Code',\n",
       "    'English Language Proficiency Level',\n",
       "    'Migrant Status',\n",
       "    'Primary Disability Type']),\n",
       "  ('numeric',\n",
       "   Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                   ('imputer', SimpleImputer(strategy='median'))]),\n",
       "   ['Reading Fall Percentile',\n",
       "    'Reading Winter Percentile',\n",
       "    'Reading Fall Score to Winter Growth',\n",
       "    'Math Fall Percentile',\n",
       "    'Math Winter Percentile',\n",
       "    'Math Fall to Winter Growth',\n",
       "    'Math Met Winter Goal?',\n",
       "    'First Entry Date Into US School',\n",
       "    'LEP Entry Date',\n",
       "    'LEP Exit Date'])],\n",
       " 'preprocessing__verbose': False,\n",
       " 'preprocessing__boolean': Pipeline(steps=[('imputer',\n",
       "                  SimpleImputer(fill_value='False', strategy='constant')),\n",
       "                 ('ohe', OneHotEncoder(handle_unknown='ignore'))]),\n",
       " 'preprocessing__categorical': Pipeline(steps=[('imputer',\n",
       "                  SimpleImputer(fill_value='missing', strategy='constant')),\n",
       "                 ('ohe', OneHotEncoder(handle_unknown='ignore'))]),\n",
       " 'preprocessing__numeric': Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                 ('imputer', SimpleImputer(strategy='median'))]),\n",
       " 'preprocessing__boolean__memory': None,\n",
       " 'preprocessing__boolean__steps': [('imputer',\n",
       "   SimpleImputer(fill_value='False', strategy='constant')),\n",
       "  ('ohe', OneHotEncoder(handle_unknown='ignore'))],\n",
       " 'preprocessing__boolean__verbose': False,\n",
       " 'preprocessing__boolean__imputer': SimpleImputer(fill_value='False', strategy='constant'),\n",
       " 'preprocessing__boolean__ohe': OneHotEncoder(handle_unknown='ignore'),\n",
       " 'preprocessing__boolean__imputer__add_indicator': False,\n",
       " 'preprocessing__boolean__imputer__copy': True,\n",
       " 'preprocessing__boolean__imputer__fill_value': 'False',\n",
       " 'preprocessing__boolean__imputer__missing_values': nan,\n",
       " 'preprocessing__boolean__imputer__strategy': 'constant',\n",
       " 'preprocessing__boolean__imputer__verbose': 0,\n",
       " 'preprocessing__boolean__ohe__categories': 'auto',\n",
       " 'preprocessing__boolean__ohe__drop': None,\n",
       " 'preprocessing__boolean__ohe__dtype': numpy.float64,\n",
       " 'preprocessing__boolean__ohe__handle_unknown': 'ignore',\n",
       " 'preprocessing__boolean__ohe__sparse': True,\n",
       " 'preprocessing__categorical__memory': None,\n",
       " 'preprocessing__categorical__steps': [('imputer',\n",
       "   SimpleImputer(fill_value='missing', strategy='constant')),\n",
       "  ('ohe', OneHotEncoder(handle_unknown='ignore'))],\n",
       " 'preprocessing__categorical__verbose': False,\n",
       " 'preprocessing__categorical__imputer': SimpleImputer(fill_value='missing', strategy='constant'),\n",
       " 'preprocessing__categorical__ohe': OneHotEncoder(handle_unknown='ignore'),\n",
       " 'preprocessing__categorical__imputer__add_indicator': False,\n",
       " 'preprocessing__categorical__imputer__copy': True,\n",
       " 'preprocessing__categorical__imputer__fill_value': 'missing',\n",
       " 'preprocessing__categorical__imputer__missing_values': nan,\n",
       " 'preprocessing__categorical__imputer__strategy': 'constant',\n",
       " 'preprocessing__categorical__imputer__verbose': 0,\n",
       " 'preprocessing__categorical__ohe__categories': 'auto',\n",
       " 'preprocessing__categorical__ohe__drop': None,\n",
       " 'preprocessing__categorical__ohe__dtype': numpy.float64,\n",
       " 'preprocessing__categorical__ohe__handle_unknown': 'ignore',\n",
       " 'preprocessing__categorical__ohe__sparse': True,\n",
       " 'preprocessing__numeric__memory': None,\n",
       " 'preprocessing__numeric__steps': [('scaler', StandardScaler()),\n",
       "  ('imputer', SimpleImputer(strategy='median'))],\n",
       " 'preprocessing__numeric__verbose': False,\n",
       " 'preprocessing__numeric__scaler': StandardScaler(),\n",
       " 'preprocessing__numeric__imputer': SimpleImputer(strategy='median'),\n",
       " 'preprocessing__numeric__scaler__copy': True,\n",
       " 'preprocessing__numeric__scaler__with_mean': True,\n",
       " 'preprocessing__numeric__scaler__with_std': True,\n",
       " 'preprocessing__numeric__imputer__add_indicator': False,\n",
       " 'preprocessing__numeric__imputer__copy': True,\n",
       " 'preprocessing__numeric__imputer__fill_value': None,\n",
       " 'preprocessing__numeric__imputer__missing_values': nan,\n",
       " 'preprocessing__numeric__imputer__strategy': 'median',\n",
       " 'preprocessing__numeric__imputer__verbose': 0,\n",
       " 'classifier__C': 10000.0,\n",
       " 'classifier__class_weight': 'balanced',\n",
       " 'classifier__dual': False,\n",
       " 'classifier__fit_intercept': False,\n",
       " 'classifier__intercept_scaling': 1,\n",
       " 'classifier__l1_ratio': None,\n",
       " 'classifier__max_iter': 500,\n",
       " 'classifier__multi_class': 'multinomial',\n",
       " 'classifier__n_jobs': None,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__random_state': None,\n",
       " 'classifier__solver': 'saga',\n",
       " 'classifier__tol': 0.0001,\n",
       " 'classifier__verbose': 0,\n",
       " 'classifier__warm_start': False}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Search for hyperparameters \n",
    "\n",
    "pipe = Pipeline([('preprocessing', preprocessing), \n",
    "                 ('classifier',  LogisticRegression())])\n",
    "\n",
    "search_space = {'classifier__C': np.logspace(0, 4, 10),\n",
    "                'classifier__class_weight': [None,'balanced'],\n",
    "                'classifier__dual': [True,False],\n",
    "                'classifier__fit_intercept': [True,False],\n",
    "                'classifier__max_iter': [10, 100, 500],\n",
    "                'classifier__multi_class': ['auto', 'ovr', 'multinomial'],\n",
    "                'classifier__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "                'classifier__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n",
    "\n",
    "clf_rand = RandomizedSearchCV(estimator=pipe, \n",
    "                            param_distributions=search_space, \n",
    "                            n_iter=5,\n",
    "                            cv=5,\n",
    "                            verbose=True)\n",
    "\n",
    "clf_rand.fit(X_train, y_train)\n",
    "clf_rand.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include only non-default params \n",
    "lr_best_params = {'C': 7.742636826811269,\n",
    "                  'class_weight': 'balanced',\n",
    "                  'fit_intercept': False,\n",
    "                  'max_iter': 500,\n",
    "                  'penalty': 'l1',\n",
    "                  'solver': 'liblinear'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish kfold and scoring to be used for all 3 candidate models\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=46)\n",
    "\n",
    "scoring = ['precision', 'accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Logistic Regression Precision: 0.905\n",
      "Mean Logistic Regression Accuracy: 0.862\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "\n",
    "pipe = Pipeline([('preprocessing', preprocessing), \n",
    "                 ('classifier', LogisticRegression(**lr_best_params))])\n",
    "\n",
    "scores = cross_validate(pipe,\n",
    "                        X_train,\n",
    "                        y_train, \n",
    "                        cv=kfold, \n",
    "                        scoring=scoring)\n",
    "\n",
    "lr_precision_score = scores['test_precision'].mean()\n",
    "lr_acc_score = scores['test_accuracy'].mean()\n",
    "print(\"Mean Logistic Regression Precision:\", round(lr_precision_score,3))\n",
    "print(\"Mean Logistic Regression Accuracy:\", round(lr_acc_score,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Model 2 : Extra Trees Classifier \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    4.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('preprocessing',\n",
       "   ColumnTransformer(transformers=[('boolean',\n",
       "                                    Pipeline(steps=[('imputer',\n",
       "                                                     SimpleImputer(fill_value='False',\n",
       "                                                                   strategy='constant')),\n",
       "                                                    ('ohe',\n",
       "                                                     OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                    ['IDEA Indicator', 'LEP Status',\n",
       "                                     'Economically Disadvantaged Status']),\n",
       "                                   ('categorical',\n",
       "                                    Pipeline(steps=[('imputer',\n",
       "                                                     SimpleImputer(fill_value='missing',\n",
       "                                                                   strategy='constant')),\n",
       "                                                    ('ohe',\n",
       "                                                     OneHo...\n",
       "                                     'Primary Disability Type']),\n",
       "                                   ('numeric',\n",
       "                                    Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                                    ('imputer',\n",
       "                                                     SimpleImputer(strategy='median'))]),\n",
       "                                    ['Reading Fall Percentile',\n",
       "                                     'Reading Winter Percentile',\n",
       "                                     'Reading Fall Score to Winter Growth',\n",
       "                                     'Math Fall Percentile',\n",
       "                                     'Math Winter Percentile',\n",
       "                                     'Math Fall to Winter Growth',\n",
       "                                     'Math Met Winter Goal?',\n",
       "                                     'First Entry Date Into US School',\n",
       "                                     'LEP Entry Date', 'LEP Exit Date'])])),\n",
       "  ('classifier',\n",
       "   ExtraTreesClassifier(bootstrap=True, ccp_alpha=0, max_samples=0.25,\n",
       "                        min_samples_leaf=3, min_weight_fraction_leaf=0.1,\n",
       "                        n_estimators=10))],\n",
       " 'verbose': False,\n",
       " 'preprocessing': ColumnTransformer(transformers=[('boolean',\n",
       "                                  Pipeline(steps=[('imputer',\n",
       "                                                   SimpleImputer(fill_value='False',\n",
       "                                                                 strategy='constant')),\n",
       "                                                  ('ohe',\n",
       "                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                  ['IDEA Indicator', 'LEP Status',\n",
       "                                   'Economically Disadvantaged Status']),\n",
       "                                 ('categorical',\n",
       "                                  Pipeline(steps=[('imputer',\n",
       "                                                   SimpleImputer(fill_value='missing',\n",
       "                                                                 strategy='constant')),\n",
       "                                                  ('ohe',\n",
       "                                                   OneHo...\n",
       "                                   'Primary Disability Type']),\n",
       "                                 ('numeric',\n",
       "                                  Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                                  ('imputer',\n",
       "                                                   SimpleImputer(strategy='median'))]),\n",
       "                                  ['Reading Fall Percentile',\n",
       "                                   'Reading Winter Percentile',\n",
       "                                   'Reading Fall Score to Winter Growth',\n",
       "                                   'Math Fall Percentile',\n",
       "                                   'Math Winter Percentile',\n",
       "                                   'Math Fall to Winter Growth',\n",
       "                                   'Math Met Winter Goal?',\n",
       "                                   'First Entry Date Into US School',\n",
       "                                   'LEP Entry Date', 'LEP Exit Date'])]),\n",
       " 'classifier': ExtraTreesClassifier(bootstrap=True, ccp_alpha=0, max_samples=0.25,\n",
       "                      min_samples_leaf=3, min_weight_fraction_leaf=0.1,\n",
       "                      n_estimators=10),\n",
       " 'preprocessing__n_jobs': None,\n",
       " 'preprocessing__remainder': 'drop',\n",
       " 'preprocessing__sparse_threshold': 0.3,\n",
       " 'preprocessing__transformer_weights': None,\n",
       " 'preprocessing__transformers': [('boolean',\n",
       "   Pipeline(steps=[('imputer',\n",
       "                    SimpleImputer(fill_value='False', strategy='constant')),\n",
       "                   ('ohe', OneHotEncoder(handle_unknown='ignore'))]),\n",
       "   ['IDEA Indicator', 'LEP Status', 'Economically Disadvantaged Status']),\n",
       "  ('categorical',\n",
       "   Pipeline(steps=[('imputer',\n",
       "                    SimpleImputer(fill_value='missing', strategy='constant')),\n",
       "                   ('ohe', OneHotEncoder(handle_unknown='ignore'))]),\n",
       "   ['Grade _x',\n",
       "    'Reading \\nMet Winter Goal?',\n",
       "    'Math Met Winter Goal?',\n",
       "    'Race/Ethnicity',\n",
       "    'Language Code',\n",
       "    'English Language Proficiency Level',\n",
       "    'Migrant Status',\n",
       "    'Primary Disability Type']),\n",
       "  ('numeric',\n",
       "   Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                   ('imputer', SimpleImputer(strategy='median'))]),\n",
       "   ['Reading Fall Percentile',\n",
       "    'Reading Winter Percentile',\n",
       "    'Reading Fall Score to Winter Growth',\n",
       "    'Math Fall Percentile',\n",
       "    'Math Winter Percentile',\n",
       "    'Math Fall to Winter Growth',\n",
       "    'Math Met Winter Goal?',\n",
       "    'First Entry Date Into US School',\n",
       "    'LEP Entry Date',\n",
       "    'LEP Exit Date'])],\n",
       " 'preprocessing__verbose': False,\n",
       " 'preprocessing__boolean': Pipeline(steps=[('imputer',\n",
       "                  SimpleImputer(fill_value='False', strategy='constant')),\n",
       "                 ('ohe', OneHotEncoder(handle_unknown='ignore'))]),\n",
       " 'preprocessing__categorical': Pipeline(steps=[('imputer',\n",
       "                  SimpleImputer(fill_value='missing', strategy='constant')),\n",
       "                 ('ohe', OneHotEncoder(handle_unknown='ignore'))]),\n",
       " 'preprocessing__numeric': Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                 ('imputer', SimpleImputer(strategy='median'))]),\n",
       " 'preprocessing__boolean__memory': None,\n",
       " 'preprocessing__boolean__steps': [('imputer',\n",
       "   SimpleImputer(fill_value='False', strategy='constant')),\n",
       "  ('ohe', OneHotEncoder(handle_unknown='ignore'))],\n",
       " 'preprocessing__boolean__verbose': False,\n",
       " 'preprocessing__boolean__imputer': SimpleImputer(fill_value='False', strategy='constant'),\n",
       " 'preprocessing__boolean__ohe': OneHotEncoder(handle_unknown='ignore'),\n",
       " 'preprocessing__boolean__imputer__add_indicator': False,\n",
       " 'preprocessing__boolean__imputer__copy': True,\n",
       " 'preprocessing__boolean__imputer__fill_value': 'False',\n",
       " 'preprocessing__boolean__imputer__missing_values': nan,\n",
       " 'preprocessing__boolean__imputer__strategy': 'constant',\n",
       " 'preprocessing__boolean__imputer__verbose': 0,\n",
       " 'preprocessing__boolean__ohe__categories': 'auto',\n",
       " 'preprocessing__boolean__ohe__drop': None,\n",
       " 'preprocessing__boolean__ohe__dtype': numpy.float64,\n",
       " 'preprocessing__boolean__ohe__handle_unknown': 'ignore',\n",
       " 'preprocessing__boolean__ohe__sparse': True,\n",
       " 'preprocessing__categorical__memory': None,\n",
       " 'preprocessing__categorical__steps': [('imputer',\n",
       "   SimpleImputer(fill_value='missing', strategy='constant')),\n",
       "  ('ohe', OneHotEncoder(handle_unknown='ignore'))],\n",
       " 'preprocessing__categorical__verbose': False,\n",
       " 'preprocessing__categorical__imputer': SimpleImputer(fill_value='missing', strategy='constant'),\n",
       " 'preprocessing__categorical__ohe': OneHotEncoder(handle_unknown='ignore'),\n",
       " 'preprocessing__categorical__imputer__add_indicator': False,\n",
       " 'preprocessing__categorical__imputer__copy': True,\n",
       " 'preprocessing__categorical__imputer__fill_value': 'missing',\n",
       " 'preprocessing__categorical__imputer__missing_values': nan,\n",
       " 'preprocessing__categorical__imputer__strategy': 'constant',\n",
       " 'preprocessing__categorical__imputer__verbose': 0,\n",
       " 'preprocessing__categorical__ohe__categories': 'auto',\n",
       " 'preprocessing__categorical__ohe__drop': None,\n",
       " 'preprocessing__categorical__ohe__dtype': numpy.float64,\n",
       " 'preprocessing__categorical__ohe__handle_unknown': 'ignore',\n",
       " 'preprocessing__categorical__ohe__sparse': True,\n",
       " 'preprocessing__numeric__memory': None,\n",
       " 'preprocessing__numeric__steps': [('scaler', StandardScaler()),\n",
       "  ('imputer', SimpleImputer(strategy='median'))],\n",
       " 'preprocessing__numeric__verbose': False,\n",
       " 'preprocessing__numeric__scaler': StandardScaler(),\n",
       " 'preprocessing__numeric__imputer': SimpleImputer(strategy='median'),\n",
       " 'preprocessing__numeric__scaler__copy': True,\n",
       " 'preprocessing__numeric__scaler__with_mean': True,\n",
       " 'preprocessing__numeric__scaler__with_std': True,\n",
       " 'preprocessing__numeric__imputer__add_indicator': False,\n",
       " 'preprocessing__numeric__imputer__copy': True,\n",
       " 'preprocessing__numeric__imputer__fill_value': None,\n",
       " 'preprocessing__numeric__imputer__missing_values': nan,\n",
       " 'preprocessing__numeric__imputer__strategy': 'median',\n",
       " 'preprocessing__numeric__imputer__verbose': 0,\n",
       " 'classifier__bootstrap': True,\n",
       " 'classifier__ccp_alpha': 0,\n",
       " 'classifier__class_weight': None,\n",
       " 'classifier__criterion': 'gini',\n",
       " 'classifier__max_depth': None,\n",
       " 'classifier__max_features': 'auto',\n",
       " 'classifier__max_leaf_nodes': None,\n",
       " 'classifier__max_samples': 0.25,\n",
       " 'classifier__min_impurity_decrease': 0.0,\n",
       " 'classifier__min_impurity_split': None,\n",
       " 'classifier__min_samples_leaf': 3,\n",
       " 'classifier__min_samples_split': 2,\n",
       " 'classifier__min_weight_fraction_leaf': 0.1,\n",
       " 'classifier__n_estimators': 10,\n",
       " 'classifier__n_jobs': None,\n",
       " 'classifier__oob_score': False,\n",
       " 'classifier__random_state': None,\n",
       " 'classifier__verbose': 0,\n",
       " 'classifier__warm_start': False}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Search for hyperparameters \n",
    "\n",
    "pipe = Pipeline([('preprocessing', preprocessing), \n",
    "                 ('classifier',  ExtraTreesClassifier())])\n",
    "\n",
    "search_space = {'classifier__bootstrap': [False,True],\n",
    "                'classifier__ccp_alpha': [0, 0.1, 0.01],\n",
    "                'classifier__class_weight': [None, 'balanced', 'balanced_subsample'],\n",
    "                'classifier__criterion': ['gini', 'entropy'],\n",
    "                'classifier__max_features': ['auto','sqrt', 'log2'],\n",
    "                'classifier__max_samples': [0.25,0.5, 0.75, None],\n",
    "                'classifier__min_samples_leaf': [1,2,3],\n",
    "                'classifier__min_samples_split': [2,3],\n",
    "                'classifier__min_weight_fraction_leaf': [0.0, 0.1],\n",
    "                'classifier__n_estimators': [10,100,500]}\n",
    "\n",
    "clf_rand = RandomizedSearchCV(estimator=pipe, \n",
    "                            param_distributions=search_space, \n",
    "                            n_iter=5,\n",
    "                            cv=5,\n",
    "                            verbose=True,\n",
    "                            )\n",
    "clf_rand.fit(X_train, y_train)\n",
    "clf_rand.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only include non-default params \n",
    "\n",
    "et_best_params = {'bootstrap': True,\n",
    "                  'min_samples_leaf': 2,\n",
    "                  'min_samples_split': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Extra Trees Precision: 0.88\n",
      "Mean Extra Trees Accuracy: 0.865\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "\n",
    "pipe = Pipeline([('preprocessing', preprocessing), \n",
    "                 ('classifier', ExtraTreesClassifier(**et_best_params))])\n",
    "\n",
    "scores = cross_validate(pipe,\n",
    "                        X_train,\n",
    "                        y_train, \n",
    "                        cv=kfold, \n",
    "                        scoring=scoring)\n",
    "\n",
    "et_precision_score = scores['test_precision'].mean()\n",
    "et_acc_score = scores['test_accuracy'].mean()\n",
    "print(\"Mean Extra Trees Precision:\", round(et_precision_score,3))\n",
    "print(\"Mean Extra Trees Accuracy:\", round(et_acc_score,3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: SGD Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('preprocessing',\n",
       "   ColumnTransformer(transformers=[('boolean',\n",
       "                                    Pipeline(steps=[('imputer',\n",
       "                                                     SimpleImputer(fill_value='False',\n",
       "                                                                   strategy='constant')),\n",
       "                                                    ('ohe',\n",
       "                                                     OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                    ['IDEA Indicator', 'LEP Status',\n",
       "                                     'Economically Disadvantaged Status']),\n",
       "                                   ('categorical',\n",
       "                                    Pipeline(steps=[('imputer',\n",
       "                                                     SimpleImputer(fill_value='missing',\n",
       "                                                                   strategy='constant')),\n",
       "                                                    ('ohe',\n",
       "                                                     OneHo...\n",
       "                                     'Primary Disability Type']),\n",
       "                                   ('numeric',\n",
       "                                    Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                                    ('imputer',\n",
       "                                                     SimpleImputer(strategy='median'))]),\n",
       "                                    ['Reading Fall Percentile',\n",
       "                                     'Reading Winter Percentile',\n",
       "                                     'Reading Fall Score to Winter Growth',\n",
       "                                     'Math Fall Percentile',\n",
       "                                     'Math Winter Percentile',\n",
       "                                     'Math Fall to Winter Growth',\n",
       "                                     'Math Met Winter Goal?',\n",
       "                                     'First Entry Date Into US School',\n",
       "                                     'LEP Entry Date', 'LEP Exit Date'])])),\n",
       "  ('classifier',\n",
       "   SGDClassifier(alpha=0.001, class_weight='balanced', early_stopping=True,\n",
       "                 fit_intercept=False))],\n",
       " 'verbose': False,\n",
       " 'preprocessing': ColumnTransformer(transformers=[('boolean',\n",
       "                                  Pipeline(steps=[('imputer',\n",
       "                                                   SimpleImputer(fill_value='False',\n",
       "                                                                 strategy='constant')),\n",
       "                                                  ('ohe',\n",
       "                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                  ['IDEA Indicator', 'LEP Status',\n",
       "                                   'Economically Disadvantaged Status']),\n",
       "                                 ('categorical',\n",
       "                                  Pipeline(steps=[('imputer',\n",
       "                                                   SimpleImputer(fill_value='missing',\n",
       "                                                                 strategy='constant')),\n",
       "                                                  ('ohe',\n",
       "                                                   OneHo...\n",
       "                                   'Primary Disability Type']),\n",
       "                                 ('numeric',\n",
       "                                  Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                                  ('imputer',\n",
       "                                                   SimpleImputer(strategy='median'))]),\n",
       "                                  ['Reading Fall Percentile',\n",
       "                                   'Reading Winter Percentile',\n",
       "                                   'Reading Fall Score to Winter Growth',\n",
       "                                   'Math Fall Percentile',\n",
       "                                   'Math Winter Percentile',\n",
       "                                   'Math Fall to Winter Growth',\n",
       "                                   'Math Met Winter Goal?',\n",
       "                                   'First Entry Date Into US School',\n",
       "                                   'LEP Entry Date', 'LEP Exit Date'])]),\n",
       " 'classifier': SGDClassifier(alpha=0.001, class_weight='balanced', early_stopping=True,\n",
       "               fit_intercept=False),\n",
       " 'preprocessing__n_jobs': None,\n",
       " 'preprocessing__remainder': 'drop',\n",
       " 'preprocessing__sparse_threshold': 0.3,\n",
       " 'preprocessing__transformer_weights': None,\n",
       " 'preprocessing__transformers': [('boolean',\n",
       "   Pipeline(steps=[('imputer',\n",
       "                    SimpleImputer(fill_value='False', strategy='constant')),\n",
       "                   ('ohe', OneHotEncoder(handle_unknown='ignore'))]),\n",
       "   ['IDEA Indicator', 'LEP Status', 'Economically Disadvantaged Status']),\n",
       "  ('categorical',\n",
       "   Pipeline(steps=[('imputer',\n",
       "                    SimpleImputer(fill_value='missing', strategy='constant')),\n",
       "                   ('ohe', OneHotEncoder(handle_unknown='ignore'))]),\n",
       "   ['Grade _x',\n",
       "    'Reading \\nMet Winter Goal?',\n",
       "    'Math Met Winter Goal?',\n",
       "    'Race/Ethnicity',\n",
       "    'Language Code',\n",
       "    'English Language Proficiency Level',\n",
       "    'Migrant Status',\n",
       "    'Primary Disability Type']),\n",
       "  ('numeric',\n",
       "   Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                   ('imputer', SimpleImputer(strategy='median'))]),\n",
       "   ['Reading Fall Percentile',\n",
       "    'Reading Winter Percentile',\n",
       "    'Reading Fall Score to Winter Growth',\n",
       "    'Math Fall Percentile',\n",
       "    'Math Winter Percentile',\n",
       "    'Math Fall to Winter Growth',\n",
       "    'Math Met Winter Goal?',\n",
       "    'First Entry Date Into US School',\n",
       "    'LEP Entry Date',\n",
       "    'LEP Exit Date'])],\n",
       " 'preprocessing__verbose': False,\n",
       " 'preprocessing__boolean': Pipeline(steps=[('imputer',\n",
       "                  SimpleImputer(fill_value='False', strategy='constant')),\n",
       "                 ('ohe', OneHotEncoder(handle_unknown='ignore'))]),\n",
       " 'preprocessing__categorical': Pipeline(steps=[('imputer',\n",
       "                  SimpleImputer(fill_value='missing', strategy='constant')),\n",
       "                 ('ohe', OneHotEncoder(handle_unknown='ignore'))]),\n",
       " 'preprocessing__numeric': Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                 ('imputer', SimpleImputer(strategy='median'))]),\n",
       " 'preprocessing__boolean__memory': None,\n",
       " 'preprocessing__boolean__steps': [('imputer',\n",
       "   SimpleImputer(fill_value='False', strategy='constant')),\n",
       "  ('ohe', OneHotEncoder(handle_unknown='ignore'))],\n",
       " 'preprocessing__boolean__verbose': False,\n",
       " 'preprocessing__boolean__imputer': SimpleImputer(fill_value='False', strategy='constant'),\n",
       " 'preprocessing__boolean__ohe': OneHotEncoder(handle_unknown='ignore'),\n",
       " 'preprocessing__boolean__imputer__add_indicator': False,\n",
       " 'preprocessing__boolean__imputer__copy': True,\n",
       " 'preprocessing__boolean__imputer__fill_value': 'False',\n",
       " 'preprocessing__boolean__imputer__missing_values': nan,\n",
       " 'preprocessing__boolean__imputer__strategy': 'constant',\n",
       " 'preprocessing__boolean__imputer__verbose': 0,\n",
       " 'preprocessing__boolean__ohe__categories': 'auto',\n",
       " 'preprocessing__boolean__ohe__drop': None,\n",
       " 'preprocessing__boolean__ohe__dtype': numpy.float64,\n",
       " 'preprocessing__boolean__ohe__handle_unknown': 'ignore',\n",
       " 'preprocessing__boolean__ohe__sparse': True,\n",
       " 'preprocessing__categorical__memory': None,\n",
       " 'preprocessing__categorical__steps': [('imputer',\n",
       "   SimpleImputer(fill_value='missing', strategy='constant')),\n",
       "  ('ohe', OneHotEncoder(handle_unknown='ignore'))],\n",
       " 'preprocessing__categorical__verbose': False,\n",
       " 'preprocessing__categorical__imputer': SimpleImputer(fill_value='missing', strategy='constant'),\n",
       " 'preprocessing__categorical__ohe': OneHotEncoder(handle_unknown='ignore'),\n",
       " 'preprocessing__categorical__imputer__add_indicator': False,\n",
       " 'preprocessing__categorical__imputer__copy': True,\n",
       " 'preprocessing__categorical__imputer__fill_value': 'missing',\n",
       " 'preprocessing__categorical__imputer__missing_values': nan,\n",
       " 'preprocessing__categorical__imputer__strategy': 'constant',\n",
       " 'preprocessing__categorical__imputer__verbose': 0,\n",
       " 'preprocessing__categorical__ohe__categories': 'auto',\n",
       " 'preprocessing__categorical__ohe__drop': None,\n",
       " 'preprocessing__categorical__ohe__dtype': numpy.float64,\n",
       " 'preprocessing__categorical__ohe__handle_unknown': 'ignore',\n",
       " 'preprocessing__categorical__ohe__sparse': True,\n",
       " 'preprocessing__numeric__memory': None,\n",
       " 'preprocessing__numeric__steps': [('scaler', StandardScaler()),\n",
       "  ('imputer', SimpleImputer(strategy='median'))],\n",
       " 'preprocessing__numeric__verbose': False,\n",
       " 'preprocessing__numeric__scaler': StandardScaler(),\n",
       " 'preprocessing__numeric__imputer': SimpleImputer(strategy='median'),\n",
       " 'preprocessing__numeric__scaler__copy': True,\n",
       " 'preprocessing__numeric__scaler__with_mean': True,\n",
       " 'preprocessing__numeric__scaler__with_std': True,\n",
       " 'preprocessing__numeric__imputer__add_indicator': False,\n",
       " 'preprocessing__numeric__imputer__copy': True,\n",
       " 'preprocessing__numeric__imputer__fill_value': None,\n",
       " 'preprocessing__numeric__imputer__missing_values': nan,\n",
       " 'preprocessing__numeric__imputer__strategy': 'median',\n",
       " 'preprocessing__numeric__imputer__verbose': 0,\n",
       " 'classifier__alpha': 0.001,\n",
       " 'classifier__average': False,\n",
       " 'classifier__class_weight': 'balanced',\n",
       " 'classifier__early_stopping': True,\n",
       " 'classifier__epsilon': 0.1,\n",
       " 'classifier__eta0': 0.0,\n",
       " 'classifier__fit_intercept': False,\n",
       " 'classifier__l1_ratio': 0.15,\n",
       " 'classifier__learning_rate': 'optimal',\n",
       " 'classifier__loss': 'hinge',\n",
       " 'classifier__max_iter': 1000,\n",
       " 'classifier__n_iter_no_change': 5,\n",
       " 'classifier__n_jobs': None,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__power_t': 0.5,\n",
       " 'classifier__random_state': None,\n",
       " 'classifier__shuffle': True,\n",
       " 'classifier__tol': 0.001,\n",
       " 'classifier__validation_fraction': 0.1,\n",
       " 'classifier__verbose': 0,\n",
       " 'classifier__warm_start': False}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Search for hyperparameters \n",
    "\n",
    "pipe = Pipeline([('preprocessing', preprocessing), \n",
    "                 ('classifier',  SGDClassifier())])\n",
    "\n",
    "search_space = {'classifier__alpha': [0.0001, 0.001, 0.01],\n",
    "                 'classifier__class_weight': [None, 'balanced'],\n",
    "                 'classifier__early_stopping': [True,False],\n",
    "                 'classifier__fit_intercept': [True, False],\n",
    "                 'classifier__l1_ratio': [0.05, 0.15, 0.25],\n",
    "                 'classifier__loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "                 'classifier__max_iter': [1000, 2000],\n",
    "                 'classifier__penalty': ['l2', 'l1', 'elasticnet']}\n",
    "\n",
    "clf_rand = RandomizedSearchCV(estimator=pipe, \n",
    "                            param_distributions=search_space, \n",
    "                            n_iter=5,\n",
    "                            cv=5,\n",
    "                            verbose=True,\n",
    "                            )\n",
    "clf_rand.fit(X_train, y_train)\n",
    "clf_rand.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include only those that are different from the default values \n",
    "\n",
    "sgd_best_params =  {'alpha': 0.01,\n",
    "                    'early_stopping': True,\n",
    "                    'fit_intercept': False,\n",
    "                    'l1_ratio': 0.25,\n",
    "                    'max_iter': 2000,\n",
    "                    'penalty': 'l1'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean SGD Precision: 0.89\n",
      "Mean SGD Accuracy: 0.844\n"
     ]
    }
   ],
   "source": [
    "# Fit model using KFold CV\n",
    "\n",
    "pipe = Pipeline([('preprocessing', preprocessing), \n",
    "                 ('classifier', SGDClassifier(**sgd_best_params))])\n",
    "\n",
    "scores = cross_validate(pipe,\n",
    "                        X_train,\n",
    "                        y_train, \n",
    "                        cv=kfold, \n",
    "                        scoring=scoring)\n",
    "\n",
    "sgd_precision_score = scores['test_precision'].mean()\n",
    "sgd_acc_score = scores['test_accuracy'].mean()\n",
    "print(\"Mean SGD Precision:\", round(sgd_precision_score,3))\n",
    "print(\"Mean SGD Accuracy:\", round(sgd_acc_score,3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of 3 Candidate Models \n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SGD</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Extra Trees</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Precision  Accuracy\n",
       "0  Logistic Regression      0.905     0.862\n",
       "2                  SGD      0.890     0.844\n",
       "1          Extra Trees      0.880     0.865"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\"Model\": [\"Logistic Regression\", \"Extra Trees\", \"SGD\"], \n",
    "              \"Precision\": [lr_precision_score,et_precision_score,sgd_precision_score], \n",
    "              \"Accuracy\": [lr_acc_score,et_acc_score,sgd_acc_score]})\\\n",
    "            .round(3)\\\n",
    "            .sort_values(['Precision'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Logistic Regression had the highest precision and the 2nd highest accuracy, I selected this as my final model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine Importances for Logistic Regression Model\n",
    "----\n",
    "Looking at the importances serves 2 purposes:\n",
    "   1. **Reduce the number of features in my model.** \n",
    "       - For logistic regression models, the simpler the model, the better because it will increase the generality of the model. I will use the results of the permutation importance to keep only the features that have positive non-zero importances. \n",
    "   2. **Gain a better understanding about which features are most important to the logistic regression model.** \n",
    "       - This will help me answer my second question about which features teachers should focus on that seem to be most important for predicting student performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and validation set from original training set\n",
    "\n",
    "X_train_imp, X_valid_imp, y_train_imp, y_valid_imp = train_test_split(X_train, y_train, random_state=46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('preprocessing', preprocessing), \n",
    "                 ('classifier',  LogisticRegression(**lr_best_params))])\n",
    "\n",
    "model = pipe.fit(X_train_imp, y_train_imp)\n",
    "\n",
    "r = permutation_importance(model, \n",
    "                           X_valid_imp, y_valid_imp,  \n",
    "                           n_repeats=5,\n",
    "                           random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Math Winter Percentile 0.22712328767123283\n",
      "Reading Winter Percentile 0.07753424657534244\n",
      "Math Met Winter Goal? 0.016027397260273944\n",
      "Race/Ethnicity 0.006164383561643815\n",
      "Economically Disadvantaged Status 0.003013698630136974\n",
      "LEP Status 0.0010958904109588852\n",
      "ID  0.0\n",
      "Grade _x 0.0\n",
      "Reading Fall '18 RIT 0.0\n",
      "Reading Fall Percentile 0.0\n",
      "Reading Typical RIT Growth Points 0.0\n",
      "Reading Tiered RIT Growth Points 0.0\n",
      "Reading Winter '18 GOAL Score 0.0\n",
      "Reading Winter '18 RIT 0.0\n",
      "Reading Fall Score to Winter Growth 0.0\n",
      "Reading \n",
      "Met Winter Goal? 0.0\n",
      "Reading Spring '19 GOAL Score 0.0\n",
      "Reading Spring '19 RIT 0.0\n",
      "Reading Spring '19 %ile 0.0\n",
      "Reading Fall to Spring RIT Growth 0.0\n",
      "Reading Met Spring Goal? 0.0\n",
      "Math Fall '18 RIT 0.0\n",
      "Math Fall Percentile 0.0\n",
      "Math Typical RIT Growth Points 0.0\n",
      "Math Tiered RIT Growth Points 0.0\n",
      "Math Winter '18 GOAL Score 0.0\n",
      "Math Winter '18 RIT 0.0\n",
      "Math Fall to Winter Growth 0.0\n",
      "Math Spring '19 GOAL Score 0.0\n",
      "Math Spring '19 RIT 0.0\n",
      "Math Spring '19 %ile 0.0\n",
      "Math Fall to Spring RIT Growth 0.0\n",
      "Math Met Spring Goal?  0.0\n",
      "Grade _y 0.0\n",
      "IDEA Indicator 0.0\n",
      "Section 504 Status 0.0\n",
      "English Language Proficiency Level 0.0\n",
      "Migrant Status 0.0\n",
      "First Entry Date Into US School 0.0\n",
      "LEP Entry Date 0.0\n",
      "LEP Exit Date 0.0\n",
      "Primary Disability Type 0.0\n",
      "ELA/Literacy OppNumber 0.0\n",
      "ELA/Literacy Scale Score 0.0\n",
      "Standard Error for ELA/Literacy Scale Score 0.0\n",
      "Reading Claim Achievement Category 0.0\n",
      "Writing Claim Achievement Category 0.0\n",
      "Listening Claim Achievement Category 0.0\n",
      "Research/Inquiry Claim Achievement Category 0.0\n",
      "Mathematics OppNumber 0.0\n",
      "Mathematics Scale Score 0.0\n",
      "Standard Error for Mathematics Scale Score 0.0\n",
      "Concepts and Procedures Claim Achievement Category 0.0\n",
      "Problem Solving and Modeling & Data Analysis Claim Achievement Category 0.0\n",
      "Communicating Reasoning Claim Achievement Category 0.0\n",
      "Language Code -1.7763568394002505e-17\n"
     ]
    }
   ],
   "source": [
    "# Print results of importances \n",
    "\n",
    "features = X.columns\n",
    "importances = r.importances_mean\n",
    "\n",
    "feature_importances = []\n",
    "for x in zip(features,importances):\n",
    "    feature_importances.append((x[0], x[1]))\n",
    "\n",
    "for x in sorted(feature_importances, key= lambda x: x[1], reverse=True):\n",
    "    print(x[0], x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove some correlated features and run again \n",
    "----\n",
    "I removed all features that had 0 or negative importance. The model originally had 23 features and now only has 6. \n",
    "\n",
    "Discarded features: 'English Language Proficiency Level', 'Reading \\nMet Winter Goal?', 'Grade _x','LEP Entry Date', 'Math Met Winter Goal?', 'Reading Fall Score to Winter Growth','IDEA Indicator', 'Migrant Status',\n",
    "'LEP Exit Date', 'Primary Disability Type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Logistic Regression Precision: 0.93\n",
      "Mean Logistic Regression Accuracy: 0.862\n",
      "\n",
      "Precision went from 0.905 to 0.93, an improvement of 0.025\n",
      "Accuracy went from 0.862 to 0.862, an improvement of 0.0\n"
     ]
    }
   ],
   "source": [
    "# Pipeline V2\n",
    "\n",
    "boolean_columns_final = ['LEP Status', 'Economically Disadvantaged Status']\n",
    "\n",
    "categorical_columns_final = ['Math Met Winter Goal?', 'Race/Ethnicity']\n",
    "\n",
    "numeric_columns_final = ['Reading Winter Percentile', 'Math Winter Percentile']\n",
    "\n",
    "\n",
    "boolean_pipe_final = Pipeline([('imputer', SimpleImputer(strategy='constant', fill_value='False')), \n",
    "                         ('ohe', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "categorical_pipe_final = Pipeline([('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "                             ('ohe', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "numeric_pipe_final = Pipeline([('scaler', StandardScaler()),\n",
    "                         ('imputer', SimpleImputer(strategy='median'))])\n",
    "\n",
    "preprocessing_final = ColumnTransformer([('boolean', boolean_pipe_final,  boolean_columns_final),\n",
    "                                   ('categorical', categorical_pipe_final, categorical_columns_final),\n",
    "                                   ('numeric',  numeric_pipe_final, numeric_columns_final)])  \n",
    "\n",
    "# Hyperparameters\n",
    "lr_best_params = {'C': 7.742636826811269,\n",
    "                  'class_weight': 'balanced',\n",
    "                  'fit_intercept': False,\n",
    "                  'max_iter': 500,\n",
    "                  'penalty': 'l1',\n",
    "                  'solver': 'liblinear'}\n",
    "\n",
    "# Final Pipeline\n",
    "pipe_final = Pipeline([('preprocessing', preprocessing_final), \n",
    "                 ('classifier',  LogisticRegression(**lr_best_params))])\n",
    "\n",
    "scores = cross_validate(pipe_final,\n",
    "                        X_train,\n",
    "                        y_train, \n",
    "                        cv=kfold, \n",
    "                        scoring=scoring)\n",
    "\n",
    "lr_final_precision_score = scores['test_precision'].mean()\n",
    "lr_final_acc_score = scores['test_accuracy'].mean()\n",
    "print(\"Mean Logistic Regression Precision:\", round(lr2_precision_score,3))\n",
    "print(\"Mean Logistic Regression Accuracy:\", round(lr2_acc_score,3))\n",
    "print(\"\")\n",
    "print(f\"Precision went from {round(lr_precision_score,3)} to {round(lr2_precision_score,3)}, an improvement of\", round(lr2_precision_score -lr_precision_score, 3))\n",
    "print(f\"Accuracy went from {round(lr_acc_score,3)} to {round(lr2_acc_score,3)}, an improvement of\", round(lr2_acc_score -lr_acc_score, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing features increased the precision, but accuracy stayed the same "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Model Results \n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Set Precision: 0.938\n",
      "Logistic Regression Set Accuracy: 0.856\n",
      "\n",
      "Test Set Confusion Matrix:\n",
      "[[22  4]\n",
      " [10 61]]\n"
     ]
    }
   ],
   "source": [
    "# Final Features\n",
    "boolean_columns_final = ['LEP Status', 'Economically Disadvantaged Status']\n",
    "\n",
    "categorical_columns_final = ['Math Met Winter Goal?', 'Race/Ethnicity']\n",
    "\n",
    "numeric_columns_final = ['Reading Winter Percentile', 'Math Winter Percentile']\n",
    "\n",
    "\n",
    "# Final Preprocessing Pipeline\n",
    "boolean_pipe_final = Pipeline([('imputer', SimpleImputer(strategy='constant', fill_value='False')), \n",
    "                         ('ohe', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "categorical_pipe_final = Pipeline([('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "                             ('ohe', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "numeric_pipe_final = Pipeline([('scaler', StandardScaler()),\n",
    "                         ('imputer', SimpleImputer(strategy='median'))])\n",
    "\n",
    "preprocessing_final = ColumnTransformer([('boolean', boolean_pipe_final,  boolean_columns_final),\n",
    "                                   ('categorical', categorical_pipe_final, categorical_columns_final),\n",
    "                                   ('numeric',  numeric_pipe_final, numeric_columns_final)])  \n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "lr_best_params = {'C': 7.742636826811269,\n",
    "                  'class_weight': 'balanced',\n",
    "                  'fit_intercept': False,\n",
    "                  'max_iter': 500,\n",
    "                  'penalty': 'l1',\n",
    "                  'solver': 'liblinear'}\n",
    "\n",
    "# Final Pipeline\n",
    "pipe_final = Pipeline([('preprocessing', preprocessing_final), \n",
    "                 ('classifier',  LogisticRegression(**lr_best_params))])\n",
    "\n",
    "# Fit Pipeline on entire train \n",
    "pipe_final.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "preds = pipe_final.predict(X_test)\n",
    "\n",
    "# See test set precision, accuraacy and confusion matrix \n",
    "lr_precision_score = precision_score(y_test, preds)\n",
    "lr_acc_score = accuracy_score(y_test, preds)\n",
    "lr_confusion = confusion_matrix(y_test, preds)\n",
    "\n",
    "print(\"Logistic Regression Set Precision:\", round(lr_precision_score,3))\n",
    "print(\"Logistic Regression Set Accuracy:\", round(lr_acc_score,3))\n",
    "print(\"\")\n",
    "print(\"Test Set Confusion Matrix:\")\n",
    "print(lr_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred: Fail</th>\n",
       "      <th>Pred: Pass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual: Fail</th>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual: Pass</th>\n",
       "      <td>10</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Pred: Fail  Pred: Pass\n",
       "Actual: Fail          22           4\n",
       "Actual: Pass          10          61"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create df to interpret confusion matrix \n",
    "labels = ['Fail', 'Pass']\n",
    "pd.DataFrame(lr_confusion, columns=[f'Pred: {label}' for label in labels],\n",
    "                  index=[f'Actual: {label}' for label in labels])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.85      0.76        26\n",
      "           1       0.94      0.86      0.90        71\n",
      "\n",
      "    accuracy                           0.86        97\n",
      "   macro avg       0.81      0.85      0.83        97\n",
      "weighted avg       0.87      0.86      0.86        97\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check classification report \n",
    "full_report = classification_report(y_test, preds)\n",
    "print(full_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting that the model has high precision for class 1 but low precision for class 0. Recall for both classes is about the same. Accuracy is 0.86, well above the a priori probability of 0.66. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions and Next Steps\n",
    "----\n",
    "\n",
    "My final logistic regression model does a good job of correctly predicting students who will pass the SBAC. The model was selected and tuned to minimize the chance of false positives. In the test set, only 3 of the 97 students are predicted to pass when in reality they failed. Using this model can help teachers identify students that need extra support who would have otherwise gone unnoticed.\n",
    "\n",
    "It is important to note that in the test set, 7 students were predicted to fail who in reality passed. The model incorrectly suggests that these students should be receiving extra interventions. This could be an issue because teachers have limited time and resources and these students would be unneceessarily taking away some of the resources from the students who need it most. The problem of false negatives is not as significant as the problem of false positives in this case, but it is important to be aware of it. \n",
    "\n",
    "Though the model has a 96% chance of correctly predicting if a student will pass, it has only a 75% chance of correctly predicting if a student will fail. When I performed this analyses, I recognized there was a class imbalance, but I did not think it was significant enough to address. Given the major differences in precision for the two classes, I would like to consider addressing this class imbalance issue in the future using SMOTE. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
