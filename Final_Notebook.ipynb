{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "# Predicting SBAC\n",
    "by Amee Tan "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of Contents \n",
    "----\n",
    "1. Research Question\n",
    "2. Evaluation Metrics\n",
    "3. Clean data, transform target into binary classes, ensure consistent class ratios among train and test set\n",
    "4. Search for best algorithms\n",
    "5. Tune hyperparameters for 3 candidate models\n",
    "6. Compare 3 candidate models and select final model\n",
    "7. Examine feature importances for the final model and remove features to simplify model\n",
    "8. Run final model on test set \n",
    "9. Conclusion and next steps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Research Questions \n",
    "----\n",
    "1. Can we use student demographic information and test scores throughout the year to predict whether or not a student will pass the Math SBAC at the end of the year? \n",
    "2. Which features are most important for the models' predictions? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Metrics  \n",
    "----\n",
    "2 evaluation metrics are used in this notebook: precision & accuracy \n",
    "\n",
    "**Precision:** \n",
    "- Since the goal is to produce a model that reduces false positives, precision is the best way to measure the model's performance.\n",
    "- A false positive means the model predicts that a student will pass SBAC when in reality they fail. This is problematic because it means there are students who should have received extra support during the school year did not receive it because the model did not identify them as being on track to fail.\n",
    "\n",
    "**Accuracy:** \n",
    "- It is also important to know the overall accuracy of the model. Given that the target class had a 66/34 split, it is important to make sure the model outperforms the a priori probability (meaning the model needs to achieve an accuracy score of 0.66 or higher)\n",
    "- I debated whether to use accuracy or balanced accuracy. Balanced accuracy is better when there are class imbalances, but since the target had about a 66/34 split between the two classes, I decided this was not enough of a class imbalance to merit using balanced accuracy. Furthermore, balanced accuracy is defined as the average of recall obtained on each class. Recall is not as important for this case, so I stuck with accuracy score as my second metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from   sklearn.base            import BaseEstimator\n",
    "from   sklearn.compose         import *\n",
    "from   sklearn.ensemble        import RandomForestClassifier, ExtraTreesClassifier, IsolationForest, GradientBoostingClassifier\n",
    "from   sklearn.experimental    import enable_iterative_imputer\n",
    "from   sklearn.impute          import *\n",
    "from   sklearn.inspection      import permutation_importance\n",
    "from   sklearn.linear_model    import LogisticRegression, PassiveAggressiveClassifier, RidgeClassifier, SGDClassifier\n",
    "from   sklearn.metrics         import precision_score, classification_report, accuracy_score, confusion_matrix\n",
    "from   sklearn.model_selection import cross_validate, cross_val_score, KFold, RandomizedSearchCV, train_test_split\n",
    "from   sklearn.neighbors       import *\n",
    "from   sklearn.pipeline        import Pipeline\n",
    "from   sklearn.preprocessing   import *\n",
    "from   sklearn.svm             import SVC\n",
    "from   sklearn.tree            import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data\n",
    "-----\n",
    "Due to FERPA guidelines around student privacy, I joined the two dataframes on student name, then removed the student name, student ID number, and school name columns before uploading the dataset to be viewed publically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('https://github.com/amtan20/predictSBAC/raw/main/public_student_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y dataframes \n",
    "X = data.drop(columns=['Mathematics Achievement Level','ELA/Literacy Achievement Level'])\n",
    "y = data['Mathematics Achievement Level']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up column names \n",
    "\n",
    "X.rename(columns={\"Reading Fall '18 %ile\" : \"Reading Fall Percentile\", \n",
    "                  \"Reading Winter '18 %ile\" : \"Reading Winter Percentile\", \n",
    "                  \"Math Fall '18 %ile\" : \"Math Fall Percentile\", \n",
    "                  \"Winter '18 %ile\" : \"Math Winter Percentile\",\n",
    "                  \"Reading \\nMet Winter Goal?\" : \"Reading Met Winter Goal?\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Binary Target \n",
    "-----\n",
    "The data in the target column originally has 4 levels:\n",
    "- Standard Not Met (Fail)\n",
    "- Standard Nearly Met (Fail)\n",
    "- Standard Met (Pass)\n",
    "- Standard Exceeded (Pass)\n",
    "\n",
    "For this project, I was more interested in determining if a student would pass or fail because this binary classification is more important for teachers than the multi-class classification. The first two levels indicate that a student failed. The last two levels indicate that a student passed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change y into binary numeric target \n",
    "def create_binary_target(y):\n",
    "    return np.where(y=='Standard Not Met',0, np.where(y=='Standard Nearly Met', 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use FunctionTransformer to apply the function to the target \n",
    "transformer = FunctionTransformer(create_binary_target)\n",
    "y = transformer.transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Train, Validation, and Test Sets \n",
    "-----\n",
    "There is a 68/32 split in the target class, so I wanted to make sure this ratio was preserved in the train and test set. To do this, I added in the stratefy argument to train_test_split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=46)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('X_train.csv')\n",
    "pd.DataFrame(y_train).to_csv('y_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Pipeline \n",
    "----\n",
    "Impute missing values, one hot encode categorical features, and standardize numeric features \n",
    "\n",
    "I selected 23 initial features to put in the model using my domain knowledge. There were some features that offer duplicate information, such as Math Fall Score and Math Fall Percentile. In those cases, I kept the percentile columns because a student's percentile is more useful for teachers than a student's raw score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_columns = ['IDEA Indicator', 'LEP Status', 'Economically Disadvantaged Status']\n",
    "\n",
    "categorical_columns = ['Grade _x', \n",
    "       'Reading Met Winter Goal?', \n",
    "       'Math Met Winter Goal?', \n",
    "       'Race/Ethnicity', 'Language Code',\n",
    "       'English Language Proficiency Level', 'Migrant Status',\n",
    "       'Primary Disability Type']\n",
    "\n",
    "numeric_columns = ['Reading Fall Percentile',\n",
    "       'Reading Winter Percentile', 'Reading Fall Score to Winter Growth',\n",
    "       'Math Fall Percentile',\n",
    "       'Math Winter Percentile', 'Math Fall to Winter Growth',\n",
    "       'Math Met Winter Goal?', \n",
    "       'First Entry Date Into US School', 'LEP Entry Date', 'LEP Exit Date']\n",
    "\n",
    "\n",
    "boolean_pipe = Pipeline([('imputer', SimpleImputer(strategy='constant', fill_value='False')), \n",
    "                         ('ohe', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "categorical_pipe = Pipeline([('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "                             ('ohe', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "numeric_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                         ('imputer', SimpleImputer(strategy='median'))])\n",
    "\n",
    "preprocessing = ColumnTransformer([('boolean', boolean_pipe,  boolean_columns),\n",
    "                                   ('categorical', categorical_pipe, categorical_columns),\n",
    "                                   ('numeric',  numeric_pipe, numeric_columns)])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search for best algorithms using randomized search cross validation\n",
    "----\n",
    "Since this is a classification problem, I wanted to do a standard Logistic Regression model. Additionally, I will my other 2 models using RandomizedSearchCV. Since RandomizedSearchCV returns a single best model, I will run it twice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create helper class \n",
    "class DummyEstimator(BaseEstimator):\n",
    "    \"Pass through class, methods are present but do nothing.\"\n",
    "    def fit(self): pass\n",
    "    def score(self): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessing',\n",
       "                 ColumnTransformer(transformers=[('boolean',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(fill_value='False',\n",
       "                                                                                 strategy='constant')),\n",
       "                                                                  ('ohe',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                  ['IDEA Indicator',\n",
       "                                                   'LEP Status',\n",
       "                                                   'Economically Disadvantaged '\n",
       "                                                   'Status']),\n",
       "                                                 ('categorical',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(fill_value='missing'...\n",
       "                                                  Pipeline(steps=[('scaler',\n",
       "                                                                   StandardScaler()),\n",
       "                                                                  ('imputer',\n",
       "                                                                   SimpleImputer(strategy='median'))]),\n",
       "                                                  ['Reading Fall Percentile',\n",
       "                                                   'Reading Winter Percentile',\n",
       "                                                   'Reading Fall Score to '\n",
       "                                                   'Winter Growth',\n",
       "                                                   'Math Fall Percentile',\n",
       "                                                   'Math Winter Percentile',\n",
       "                                                   'Math Fall to Winter Growth',\n",
       "                                                   'Math Met Winter Goal?',\n",
       "                                                   'First Entry Date Into US '\n",
       "                                                   'School',\n",
       "                                                   'LEP Entry Date',\n",
       "                                                   'LEP Exit Date'])])),\n",
       "                ('mod', SGDClassifier())])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1st iteration \n",
    "\n",
    "pipe = Pipeline([('preprocessing', preprocessing),\n",
    "                 ('mod', DummyEstimator())])\n",
    "\n",
    "models = [{'mod' : [ExtraTreesClassifier()]},\n",
    "          {'mod' : [GradientBoostingClassifier()]},\n",
    "          {'mod' : [KNeighborsClassifier()]},\n",
    "          {'mod' : [LogisticRegression()]},\n",
    "          {'mod' : [PassiveAggressiveClassifier()]},\n",
    "          {'mod' : [RandomForestClassifier()]},\n",
    "          {'mod' : [RidgeClassifier()]},\n",
    "          {'mod' : [SGDClassifier()]},\n",
    "          {'mod' : [SVC()]}\n",
    "          ]\n",
    "\n",
    "clf_rand = RandomizedSearchCV(estimator=pipe, \n",
    "                              param_distributions=models, \n",
    "                              n_iter=5,\n",
    "                              cv=5,\n",
    "                              scoring='precision',\n",
    "                              n_jobs=-1)\n",
    "best_model = clf_rand.fit(X_train, y_train) \n",
    "best_model.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessing',\n",
       "                 ColumnTransformer(transformers=[('boolean',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(fill_value='False',\n",
       "                                                                                 strategy='constant')),\n",
       "                                                                  ('ohe',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                  ['IDEA Indicator',\n",
       "                                                   'LEP Status',\n",
       "                                                   'Economically Disadvantaged '\n",
       "                                                   'Status']),\n",
       "                                                 ('categorical',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(fill_value='missing'...\n",
       "                                                  Pipeline(steps=[('scaler',\n",
       "                                                                   StandardScaler()),\n",
       "                                                                  ('imputer',\n",
       "                                                                   SimpleImputer(strategy='median'))]),\n",
       "                                                  ['Reading Fall Percentile',\n",
       "                                                   'Reading Winter Percentile',\n",
       "                                                   'Reading Fall Score to '\n",
       "                                                   'Winter Growth',\n",
       "                                                   'Math Fall Percentile',\n",
       "                                                   'Math Winter Percentile',\n",
       "                                                   'Math Fall to Winter Growth',\n",
       "                                                   'Math Met Winter Goal?',\n",
       "                                                   'First Entry Date Into US '\n",
       "                                                   'School',\n",
       "                                                   'LEP Entry Date',\n",
       "                                                   'LEP Exit Date'])])),\n",
       "                ('mod', ExtraTreesClassifier())])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2nd iteration \n",
    "\n",
    "pipe = Pipeline([('preprocessing', preprocessing),\n",
    "                 ('mod', DummyEstimator())])\n",
    "\n",
    "clf_rand = RandomizedSearchCV(estimator=pipe, \n",
    "                              param_distributions=models, \n",
    "                              n_iter=5,\n",
    "                              cv=5,\n",
    "                              scoring='precision',\n",
    "                              n_jobs=-1)\n",
    "best_model = clf_rand.fit(X_train, y_train) \n",
    "best_model.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper Parameter Tuning and KFold Cross Validation for each candidate model\n",
    "----\n",
    "Now that I have my 3 candidate models, I will:\n",
    "1. Do a randomized search with cross validation to find the best hyperparameters for each model. \n",
    "2. Run a KFold Cross Validation to get the precision and accuracy scores for the tuned models. I will get 10 precision and accuracy scores from the KFold Cross Validation and will take the mean. Taking the average of 10 scores will give me a more stable estimate of the performance of the model on the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    1.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('preprocessing',\n",
       "   ColumnTransformer(transformers=[('boolean',\n",
       "                                    Pipeline(steps=[('imputer',\n",
       "                                                     SimpleImputer(fill_value='False',\n",
       "                                                                   strategy='constant')),\n",
       "                                                    ('ohe',\n",
       "                                                     OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                    ['IDEA Indicator', 'LEP Status',\n",
       "                                     'Economically Disadvantaged Status']),\n",
       "                                   ('categorical',\n",
       "                                    Pipeline(steps=[('imputer',\n",
       "                                                     SimpleImputer(fill_value='missing',\n",
       "                                                                   strategy='constant')),\n",
       "                                                    ('ohe',\n",
       "                                                     OneHo...\n",
       "                                     'Primary Disability Type']),\n",
       "                                   ('numeric',\n",
       "                                    Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                                    ('imputer',\n",
       "                                                     SimpleImputer(strategy='median'))]),\n",
       "                                    ['Reading Fall Percentile',\n",
       "                                     'Reading Winter Percentile',\n",
       "                                     'Reading Fall Score to Winter Growth',\n",
       "                                     'Math Fall Percentile',\n",
       "                                     'Math Winter Percentile',\n",
       "                                     'Math Fall to Winter Growth',\n",
       "                                     'Math Met Winter Goal?',\n",
       "                                     'First Entry Date Into US School',\n",
       "                                     'LEP Entry Date', 'LEP Exit Date'])])),\n",
       "  ('classifier',\n",
       "   LogisticRegression(C=10000.0, class_weight='balanced', fit_intercept=False,\n",
       "                      max_iter=500, multi_class='multinomial', penalty='l1',\n",
       "                      solver='saga'))],\n",
       " 'verbose': False,\n",
       " 'preprocessing': ColumnTransformer(transformers=[('boolean',\n",
       "                                  Pipeline(steps=[('imputer',\n",
       "                                                   SimpleImputer(fill_value='False',\n",
       "                                                                 strategy='constant')),\n",
       "                                                  ('ohe',\n",
       "                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                  ['IDEA Indicator', 'LEP Status',\n",
       "                                   'Economically Disadvantaged Status']),\n",
       "                                 ('categorical',\n",
       "                                  Pipeline(steps=[('imputer',\n",
       "                                                   SimpleImputer(fill_value='missing',\n",
       "                                                                 strategy='constant')),\n",
       "                                                  ('ohe',\n",
       "                                                   OneHo...\n",
       "                                   'Primary Disability Type']),\n",
       "                                 ('numeric',\n",
       "                                  Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                                  ('imputer',\n",
       "                                                   SimpleImputer(strategy='median'))]),\n",
       "                                  ['Reading Fall Percentile',\n",
       "                                   'Reading Winter Percentile',\n",
       "                                   'Reading Fall Score to Winter Growth',\n",
       "                                   'Math Fall Percentile',\n",
       "                                   'Math Winter Percentile',\n",
       "                                   'Math Fall to Winter Growth',\n",
       "                                   'Math Met Winter Goal?',\n",
       "                                   'First Entry Date Into US School',\n",
       "                                   'LEP Entry Date', 'LEP Exit Date'])]),\n",
       " 'classifier': LogisticRegression(C=10000.0, class_weight='balanced', fit_intercept=False,\n",
       "                    max_iter=500, multi_class='multinomial', penalty='l1',\n",
       "                    solver='saga'),\n",
       " 'preprocessing__n_jobs': None,\n",
       " 'preprocessing__remainder': 'drop',\n",
       " 'preprocessing__sparse_threshold': 0.3,\n",
       " 'preprocessing__transformer_weights': None,\n",
       " 'preprocessing__transformers': [('boolean',\n",
       "   Pipeline(steps=[('imputer',\n",
       "                    SimpleImputer(fill_value='False', strategy='constant')),\n",
       "                   ('ohe', OneHotEncoder(handle_unknown='ignore'))]),\n",
       "   ['IDEA Indicator', 'LEP Status', 'Economically Disadvantaged Status']),\n",
       "  ('categorical',\n",
       "   Pipeline(steps=[('imputer',\n",
       "                    SimpleImputer(fill_value='missing', strategy='constant')),\n",
       "                   ('ohe', OneHotEncoder(handle_unknown='ignore'))]),\n",
       "   ['Grade _x',\n",
       "    'Reading \\nMet Winter Goal?',\n",
       "    'Math Met Winter Goal?',\n",
       "    'Race/Ethnicity',\n",
       "    'Language Code',\n",
       "    'English Language Proficiency Level',\n",
       "    'Migrant Status',\n",
       "    'Primary Disability Type']),\n",
       "  ('numeric',\n",
       "   Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                   ('imputer', SimpleImputer(strategy='median'))]),\n",
       "   ['Reading Fall Percentile',\n",
       "    'Reading Winter Percentile',\n",
       "    'Reading Fall Score to Winter Growth',\n",
       "    'Math Fall Percentile',\n",
       "    'Math Winter Percentile',\n",
       "    'Math Fall to Winter Growth',\n",
       "    'Math Met Winter Goal?',\n",
       "    'First Entry Date Into US School',\n",
       "    'LEP Entry Date',\n",
       "    'LEP Exit Date'])],\n",
       " 'preprocessing__verbose': False,\n",
       " 'preprocessing__boolean': Pipeline(steps=[('imputer',\n",
       "                  SimpleImputer(fill_value='False', strategy='constant')),\n",
       "                 ('ohe', OneHotEncoder(handle_unknown='ignore'))]),\n",
       " 'preprocessing__categorical': Pipeline(steps=[('imputer',\n",
       "                  SimpleImputer(fill_value='missing', strategy='constant')),\n",
       "                 ('ohe', OneHotEncoder(handle_unknown='ignore'))]),\n",
       " 'preprocessing__numeric': Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                 ('imputer', SimpleImputer(strategy='median'))]),\n",
       " 'preprocessing__boolean__memory': None,\n",
       " 'preprocessing__boolean__steps': [('imputer',\n",
       "   SimpleImputer(fill_value='False', strategy='constant')),\n",
       "  ('ohe', OneHotEncoder(handle_unknown='ignore'))],\n",
       " 'preprocessing__boolean__verbose': False,\n",
       " 'preprocessing__boolean__imputer': SimpleImputer(fill_value='False', strategy='constant'),\n",
       " 'preprocessing__boolean__ohe': OneHotEncoder(handle_unknown='ignore'),\n",
       " 'preprocessing__boolean__imputer__add_indicator': False,\n",
       " 'preprocessing__boolean__imputer__copy': True,\n",
       " 'preprocessing__boolean__imputer__fill_value': 'False',\n",
       " 'preprocessing__boolean__imputer__missing_values': nan,\n",
       " 'preprocessing__boolean__imputer__strategy': 'constant',\n",
       " 'preprocessing__boolean__imputer__verbose': 0,\n",
       " 'preprocessing__boolean__ohe__categories': 'auto',\n",
       " 'preprocessing__boolean__ohe__drop': None,\n",
       " 'preprocessing__boolean__ohe__dtype': numpy.float64,\n",
       " 'preprocessing__boolean__ohe__handle_unknown': 'ignore',\n",
       " 'preprocessing__boolean__ohe__sparse': True,\n",
       " 'preprocessing__categorical__memory': None,\n",
       " 'preprocessing__categorical__steps': [('imputer',\n",
       "   SimpleImputer(fill_value='missing', strategy='constant')),\n",
       "  ('ohe', OneHotEncoder(handle_unknown='ignore'))],\n",
       " 'preprocessing__categorical__verbose': False,\n",
       " 'preprocessing__categorical__imputer': SimpleImputer(fill_value='missing', strategy='constant'),\n",
       " 'preprocessing__categorical__ohe': OneHotEncoder(handle_unknown='ignore'),\n",
       " 'preprocessing__categorical__imputer__add_indicator': False,\n",
       " 'preprocessing__categorical__imputer__copy': True,\n",
       " 'preprocessing__categorical__imputer__fill_value': 'missing',\n",
       " 'preprocessing__categorical__imputer__missing_values': nan,\n",
       " 'preprocessing__categorical__imputer__strategy': 'constant',\n",
       " 'preprocessing__categorical__imputer__verbose': 0,\n",
       " 'preprocessing__categorical__ohe__categories': 'auto',\n",
       " 'preprocessing__categorical__ohe__drop': None,\n",
       " 'preprocessing__categorical__ohe__dtype': numpy.float64,\n",
       " 'preprocessing__categorical__ohe__handle_unknown': 'ignore',\n",
       " 'preprocessing__categorical__ohe__sparse': True,\n",
       " 'preprocessing__numeric__memory': None,\n",
       " 'preprocessing__numeric__steps': [('scaler', StandardScaler()),\n",
       "  ('imputer', SimpleImputer(strategy='median'))],\n",
       " 'preprocessing__numeric__verbose': False,\n",
       " 'preprocessing__numeric__scaler': StandardScaler(),\n",
       " 'preprocessing__numeric__imputer': SimpleImputer(strategy='median'),\n",
       " 'preprocessing__numeric__scaler__copy': True,\n",
       " 'preprocessing__numeric__scaler__with_mean': True,\n",
       " 'preprocessing__numeric__scaler__with_std': True,\n",
       " 'preprocessing__numeric__imputer__add_indicator': False,\n",
       " 'preprocessing__numeric__imputer__copy': True,\n",
       " 'preprocessing__numeric__imputer__fill_value': None,\n",
       " 'preprocessing__numeric__imputer__missing_values': nan,\n",
       " 'preprocessing__numeric__imputer__strategy': 'median',\n",
       " 'preprocessing__numeric__imputer__verbose': 0,\n",
       " 'classifier__C': 10000.0,\n",
       " 'classifier__class_weight': 'balanced',\n",
       " 'classifier__dual': False,\n",
       " 'classifier__fit_intercept': False,\n",
       " 'classifier__intercept_scaling': 1,\n",
       " 'classifier__l1_ratio': None,\n",
       " 'classifier__max_iter': 500,\n",
       " 'classifier__multi_class': 'multinomial',\n",
       " 'classifier__n_jobs': None,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__random_state': None,\n",
       " 'classifier__solver': 'saga',\n",
       " 'classifier__tol': 0.0001,\n",
       " 'classifier__verbose': 0,\n",
       " 'classifier__warm_start': False}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Search for hyperparameters \n",
    "\n",
    "pipe = Pipeline([('preprocessing', preprocessing), \n",
    "                 ('classifier',  LogisticRegression())])\n",
    "\n",
    "search_space = {'classifier__C': np.logspace(0, 4, 10),\n",
    "                'classifier__class_weight': [None,'balanced'],\n",
    "                'classifier__dual': [True,False],\n",
    "                'classifier__fit_intercept': [True,False],\n",
    "                'classifier__max_iter': [10, 100, 500],\n",
    "                'classifier__multi_class': ['auto', 'ovr', 'multinomial'],\n",
    "                'classifier__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "                'classifier__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n",
    "\n",
    "clf_rand = RandomizedSearchCV(estimator=pipe, \n",
    "                            param_distributions=search_space, \n",
    "                            n_iter=5,\n",
    "                            cv=5,\n",
    "                            verbose=True)\n",
    "\n",
    "clf_rand.fit(X_train, y_train)\n",
    "clf_rand.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include only non-default params \n",
    "lr_best_params = {'C': 7.742636826811269,\n",
    "                  'class_weight': 'balanced',\n",
    "                  'fit_intercept': False,\n",
    "                  'max_iter': 500,\n",
    "                  'penalty': 'l1',\n",
    "                  'solver': 'liblinear'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish kfold and scoring to be used for all 3 candidate models\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=46)\n",
    "\n",
    "scoring = ['precision', 'accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Logistic Regression Precision: 0.909\n",
      "Mean Logistic Regression Accuracy: 0.837\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "\n",
    "pipe = Pipeline([('preprocessing', preprocessing), \n",
    "                 ('classifier', LogisticRegression(**lr_best_params))])\n",
    "\n",
    "scores = cross_validate(pipe,\n",
    "                        X_train,\n",
    "                        y_train, \n",
    "                        cv=kfold, \n",
    "                        scoring=scoring)\n",
    "\n",
    "lr_precision_score = scores['test_precision'].mean()\n",
    "lr_acc_score = scores['test_accuracy'].mean()\n",
    "print(\"Mean Logistic Regression Precision:\", round(lr_precision_score,3))\n",
    "print(\"Mean Logistic Regression Accuracy:\", round(lr_acc_score,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Model 2 : Extra Trees Classifier \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    4.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('preprocessing',\n",
       "   ColumnTransformer(transformers=[('boolean',\n",
       "                                    Pipeline(steps=[('imputer',\n",
       "                                                     SimpleImputer(fill_value='False',\n",
       "                                                                   strategy='constant')),\n",
       "                                                    ('ohe',\n",
       "                                                     OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                    ['IDEA Indicator', 'LEP Status',\n",
       "                                     'Economically Disadvantaged Status']),\n",
       "                                   ('categorical',\n",
       "                                    Pipeline(steps=[('imputer',\n",
       "                                                     SimpleImputer(fill_value='missing',\n",
       "                                                                   strategy='constant')),\n",
       "                                                    ('ohe',\n",
       "                                                     OneHo...\n",
       "                                     'Primary Disability Type']),\n",
       "                                   ('numeric',\n",
       "                                    Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                                    ('imputer',\n",
       "                                                     SimpleImputer(strategy='median'))]),\n",
       "                                    ['Reading Fall Percentile',\n",
       "                                     'Reading Winter Percentile',\n",
       "                                     'Reading Fall Score to Winter Growth',\n",
       "                                     'Math Fall Percentile',\n",
       "                                     'Math Winter Percentile',\n",
       "                                     'Math Fall to Winter Growth',\n",
       "                                     'Math Met Winter Goal?',\n",
       "                                     'First Entry Date Into US School',\n",
       "                                     'LEP Entry Date', 'LEP Exit Date'])])),\n",
       "  ('classifier',\n",
       "   ExtraTreesClassifier(bootstrap=True, ccp_alpha=0, max_samples=0.25,\n",
       "                        min_samples_leaf=3, min_weight_fraction_leaf=0.1,\n",
       "                        n_estimators=10))],\n",
       " 'verbose': False,\n",
       " 'preprocessing': ColumnTransformer(transformers=[('boolean',\n",
       "                                  Pipeline(steps=[('imputer',\n",
       "                                                   SimpleImputer(fill_value='False',\n",
       "                                                                 strategy='constant')),\n",
       "                                                  ('ohe',\n",
       "                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                  ['IDEA Indicator', 'LEP Status',\n",
       "                                   'Economically Disadvantaged Status']),\n",
       "                                 ('categorical',\n",
       "                                  Pipeline(steps=[('imputer',\n",
       "                                                   SimpleImputer(fill_value='missing',\n",
       "                                                                 strategy='constant')),\n",
       "                                                  ('ohe',\n",
       "                                                   OneHo...\n",
       "                                   'Primary Disability Type']),\n",
       "                                 ('numeric',\n",
       "                                  Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                                  ('imputer',\n",
       "                                                   SimpleImputer(strategy='median'))]),\n",
       "                                  ['Reading Fall Percentile',\n",
       "                                   'Reading Winter Percentile',\n",
       "                                   'Reading Fall Score to Winter Growth',\n",
       "                                   'Math Fall Percentile',\n",
       "                                   'Math Winter Percentile',\n",
       "                                   'Math Fall to Winter Growth',\n",
       "                                   'Math Met Winter Goal?',\n",
       "                                   'First Entry Date Into US School',\n",
       "                                   'LEP Entry Date', 'LEP Exit Date'])]),\n",
       " 'classifier': ExtraTreesClassifier(bootstrap=True, ccp_alpha=0, max_samples=0.25,\n",
       "                      min_samples_leaf=3, min_weight_fraction_leaf=0.1,\n",
       "                      n_estimators=10),\n",
       " 'preprocessing__n_jobs': None,\n",
       " 'preprocessing__remainder': 'drop',\n",
       " 'preprocessing__sparse_threshold': 0.3,\n",
       " 'preprocessing__transformer_weights': None,\n",
       " 'preprocessing__transformers': [('boolean',\n",
       "   Pipeline(steps=[('imputer',\n",
       "                    SimpleImputer(fill_value='False', strategy='constant')),\n",
       "                   ('ohe', OneHotEncoder(handle_unknown='ignore'))]),\n",
       "   ['IDEA Indicator', 'LEP Status', 'Economically Disadvantaged Status']),\n",
       "  ('categorical',\n",
       "   Pipeline(steps=[('imputer',\n",
       "                    SimpleImputer(fill_value='missing', strategy='constant')),\n",
       "                   ('ohe', OneHotEncoder(handle_unknown='ignore'))]),\n",
       "   ['Grade _x',\n",
       "    'Reading \\nMet Winter Goal?',\n",
       "    'Math Met Winter Goal?',\n",
       "    'Race/Ethnicity',\n",
       "    'Language Code',\n",
       "    'English Language Proficiency Level',\n",
       "    'Migrant Status',\n",
       "    'Primary Disability Type']),\n",
       "  ('numeric',\n",
       "   Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                   ('imputer', SimpleImputer(strategy='median'))]),\n",
       "   ['Reading Fall Percentile',\n",
       "    'Reading Winter Percentile',\n",
       "    'Reading Fall Score to Winter Growth',\n",
       "    'Math Fall Percentile',\n",
       "    'Math Winter Percentile',\n",
       "    'Math Fall to Winter Growth',\n",
       "    'Math Met Winter Goal?',\n",
       "    'First Entry Date Into US School',\n",
       "    'LEP Entry Date',\n",
       "    'LEP Exit Date'])],\n",
       " 'preprocessing__verbose': False,\n",
       " 'preprocessing__boolean': Pipeline(steps=[('imputer',\n",
       "                  SimpleImputer(fill_value='False', strategy='constant')),\n",
       "                 ('ohe', OneHotEncoder(handle_unknown='ignore'))]),\n",
       " 'preprocessing__categorical': Pipeline(steps=[('imputer',\n",
       "                  SimpleImputer(fill_value='missing', strategy='constant')),\n",
       "                 ('ohe', OneHotEncoder(handle_unknown='ignore'))]),\n",
       " 'preprocessing__numeric': Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                 ('imputer', SimpleImputer(strategy='median'))]),\n",
       " 'preprocessing__boolean__memory': None,\n",
       " 'preprocessing__boolean__steps': [('imputer',\n",
       "   SimpleImputer(fill_value='False', strategy='constant')),\n",
       "  ('ohe', OneHotEncoder(handle_unknown='ignore'))],\n",
       " 'preprocessing__boolean__verbose': False,\n",
       " 'preprocessing__boolean__imputer': SimpleImputer(fill_value='False', strategy='constant'),\n",
       " 'preprocessing__boolean__ohe': OneHotEncoder(handle_unknown='ignore'),\n",
       " 'preprocessing__boolean__imputer__add_indicator': False,\n",
       " 'preprocessing__boolean__imputer__copy': True,\n",
       " 'preprocessing__boolean__imputer__fill_value': 'False',\n",
       " 'preprocessing__boolean__imputer__missing_values': nan,\n",
       " 'preprocessing__boolean__imputer__strategy': 'constant',\n",
       " 'preprocessing__boolean__imputer__verbose': 0,\n",
       " 'preprocessing__boolean__ohe__categories': 'auto',\n",
       " 'preprocessing__boolean__ohe__drop': None,\n",
       " 'preprocessing__boolean__ohe__dtype': numpy.float64,\n",
       " 'preprocessing__boolean__ohe__handle_unknown': 'ignore',\n",
       " 'preprocessing__boolean__ohe__sparse': True,\n",
       " 'preprocessing__categorical__memory': None,\n",
       " 'preprocessing__categorical__steps': [('imputer',\n",
       "   SimpleImputer(fill_value='missing', strategy='constant')),\n",
       "  ('ohe', OneHotEncoder(handle_unknown='ignore'))],\n",
       " 'preprocessing__categorical__verbose': False,\n",
       " 'preprocessing__categorical__imputer': SimpleImputer(fill_value='missing', strategy='constant'),\n",
       " 'preprocessing__categorical__ohe': OneHotEncoder(handle_unknown='ignore'),\n",
       " 'preprocessing__categorical__imputer__add_indicator': False,\n",
       " 'preprocessing__categorical__imputer__copy': True,\n",
       " 'preprocessing__categorical__imputer__fill_value': 'missing',\n",
       " 'preprocessing__categorical__imputer__missing_values': nan,\n",
       " 'preprocessing__categorical__imputer__strategy': 'constant',\n",
       " 'preprocessing__categorical__imputer__verbose': 0,\n",
       " 'preprocessing__categorical__ohe__categories': 'auto',\n",
       " 'preprocessing__categorical__ohe__drop': None,\n",
       " 'preprocessing__categorical__ohe__dtype': numpy.float64,\n",
       " 'preprocessing__categorical__ohe__handle_unknown': 'ignore',\n",
       " 'preprocessing__categorical__ohe__sparse': True,\n",
       " 'preprocessing__numeric__memory': None,\n",
       " 'preprocessing__numeric__steps': [('scaler', StandardScaler()),\n",
       "  ('imputer', SimpleImputer(strategy='median'))],\n",
       " 'preprocessing__numeric__verbose': False,\n",
       " 'preprocessing__numeric__scaler': StandardScaler(),\n",
       " 'preprocessing__numeric__imputer': SimpleImputer(strategy='median'),\n",
       " 'preprocessing__numeric__scaler__copy': True,\n",
       " 'preprocessing__numeric__scaler__with_mean': True,\n",
       " 'preprocessing__numeric__scaler__with_std': True,\n",
       " 'preprocessing__numeric__imputer__add_indicator': False,\n",
       " 'preprocessing__numeric__imputer__copy': True,\n",
       " 'preprocessing__numeric__imputer__fill_value': None,\n",
       " 'preprocessing__numeric__imputer__missing_values': nan,\n",
       " 'preprocessing__numeric__imputer__strategy': 'median',\n",
       " 'preprocessing__numeric__imputer__verbose': 0,\n",
       " 'classifier__bootstrap': True,\n",
       " 'classifier__ccp_alpha': 0,\n",
       " 'classifier__class_weight': None,\n",
       " 'classifier__criterion': 'gini',\n",
       " 'classifier__max_depth': None,\n",
       " 'classifier__max_features': 'auto',\n",
       " 'classifier__max_leaf_nodes': None,\n",
       " 'classifier__max_samples': 0.25,\n",
       " 'classifier__min_impurity_decrease': 0.0,\n",
       " 'classifier__min_impurity_split': None,\n",
       " 'classifier__min_samples_leaf': 3,\n",
       " 'classifier__min_samples_split': 2,\n",
       " 'classifier__min_weight_fraction_leaf': 0.1,\n",
       " 'classifier__n_estimators': 10,\n",
       " 'classifier__n_jobs': None,\n",
       " 'classifier__oob_score': False,\n",
       " 'classifier__random_state': None,\n",
       " 'classifier__verbose': 0,\n",
       " 'classifier__warm_start': False}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Search for hyperparameters \n",
    "\n",
    "pipe = Pipeline([('preprocessing', preprocessing), \n",
    "                 ('classifier',  ExtraTreesClassifier())])\n",
    "\n",
    "search_space = {'classifier__bootstrap': [False,True],\n",
    "                'classifier__ccp_alpha': [0, 0.1, 0.01],\n",
    "                'classifier__class_weight': [None, 'balanced', 'balanced_subsample'],\n",
    "                'classifier__criterion': ['gini', 'entropy'],\n",
    "                'classifier__max_features': ['auto','sqrt', 'log2'],\n",
    "                'classifier__max_samples': [0.25,0.5, 0.75, None],\n",
    "                'classifier__min_samples_leaf': [1,2,3],\n",
    "                'classifier__min_samples_split': [2,3],\n",
    "                'classifier__min_weight_fraction_leaf': [0.0, 0.1],\n",
    "                'classifier__n_estimators': [10,100,500]}\n",
    "\n",
    "clf_rand = RandomizedSearchCV(estimator=pipe, \n",
    "                            param_distributions=search_space, \n",
    "                            n_iter=5,\n",
    "                            cv=5,\n",
    "                            verbose=True,\n",
    "                            )\n",
    "clf_rand.fit(X_train, y_train)\n",
    "clf_rand.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only include non-default params \n",
    "\n",
    "et_best_params = {'bootstrap': True,\n",
    "                  'min_samples_leaf': 2,\n",
    "                  'min_samples_split': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Extra Trees Precision: 0.856\n",
      "Mean Extra Trees Accuracy: 0.851\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "\n",
    "pipe = Pipeline([('preprocessing', preprocessing), \n",
    "                 ('classifier', ExtraTreesClassifier(**et_best_params))])\n",
    "\n",
    "scores = cross_validate(pipe,\n",
    "                        X_train,\n",
    "                        y_train, \n",
    "                        cv=kfold, \n",
    "                        scoring=scoring)\n",
    "\n",
    "et_precision_score = scores['test_precision'].mean()\n",
    "et_acc_score = scores['test_accuracy'].mean()\n",
    "print(\"Mean Extra Trees Precision:\", round(et_precision_score,3))\n",
    "print(\"Mean Extra Trees Accuracy:\", round(et_acc_score,3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: SGD Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('preprocessing',\n",
       "   ColumnTransformer(transformers=[('boolean',\n",
       "                                    Pipeline(steps=[('imputer',\n",
       "                                                     SimpleImputer(fill_value='False',\n",
       "                                                                   strategy='constant')),\n",
       "                                                    ('ohe',\n",
       "                                                     OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                    ['IDEA Indicator', 'LEP Status',\n",
       "                                     'Economically Disadvantaged Status']),\n",
       "                                   ('categorical',\n",
       "                                    Pipeline(steps=[('imputer',\n",
       "                                                     SimpleImputer(fill_value='missing',\n",
       "                                                                   strategy='constant')),\n",
       "                                                    ('ohe',\n",
       "                                                     OneHo...\n",
       "                                     'Primary Disability Type']),\n",
       "                                   ('numeric',\n",
       "                                    Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                                    ('imputer',\n",
       "                                                     SimpleImputer(strategy='median'))]),\n",
       "                                    ['Reading Fall Percentile',\n",
       "                                     'Reading Winter Percentile',\n",
       "                                     'Reading Fall Score to Winter Growth',\n",
       "                                     'Math Fall Percentile',\n",
       "                                     'Math Winter Percentile',\n",
       "                                     'Math Fall to Winter Growth',\n",
       "                                     'Math Met Winter Goal?',\n",
       "                                     'First Entry Date Into US School',\n",
       "                                     'LEP Entry Date', 'LEP Exit Date'])])),\n",
       "  ('classifier',\n",
       "   SGDClassifier(alpha=0.001, class_weight='balanced', early_stopping=True,\n",
       "                 fit_intercept=False))],\n",
       " 'verbose': False,\n",
       " 'preprocessing': ColumnTransformer(transformers=[('boolean',\n",
       "                                  Pipeline(steps=[('imputer',\n",
       "                                                   SimpleImputer(fill_value='False',\n",
       "                                                                 strategy='constant')),\n",
       "                                                  ('ohe',\n",
       "                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                  ['IDEA Indicator', 'LEP Status',\n",
       "                                   'Economically Disadvantaged Status']),\n",
       "                                 ('categorical',\n",
       "                                  Pipeline(steps=[('imputer',\n",
       "                                                   SimpleImputer(fill_value='missing',\n",
       "                                                                 strategy='constant')),\n",
       "                                                  ('ohe',\n",
       "                                                   OneHo...\n",
       "                                   'Primary Disability Type']),\n",
       "                                 ('numeric',\n",
       "                                  Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                                  ('imputer',\n",
       "                                                   SimpleImputer(strategy='median'))]),\n",
       "                                  ['Reading Fall Percentile',\n",
       "                                   'Reading Winter Percentile',\n",
       "                                   'Reading Fall Score to Winter Growth',\n",
       "                                   'Math Fall Percentile',\n",
       "                                   'Math Winter Percentile',\n",
       "                                   'Math Fall to Winter Growth',\n",
       "                                   'Math Met Winter Goal?',\n",
       "                                   'First Entry Date Into US School',\n",
       "                                   'LEP Entry Date', 'LEP Exit Date'])]),\n",
       " 'classifier': SGDClassifier(alpha=0.001, class_weight='balanced', early_stopping=True,\n",
       "               fit_intercept=False),\n",
       " 'preprocessing__n_jobs': None,\n",
       " 'preprocessing__remainder': 'drop',\n",
       " 'preprocessing__sparse_threshold': 0.3,\n",
       " 'preprocessing__transformer_weights': None,\n",
       " 'preprocessing__transformers': [('boolean',\n",
       "   Pipeline(steps=[('imputer',\n",
       "                    SimpleImputer(fill_value='False', strategy='constant')),\n",
       "                   ('ohe', OneHotEncoder(handle_unknown='ignore'))]),\n",
       "   ['IDEA Indicator', 'LEP Status', 'Economically Disadvantaged Status']),\n",
       "  ('categorical',\n",
       "   Pipeline(steps=[('imputer',\n",
       "                    SimpleImputer(fill_value='missing', strategy='constant')),\n",
       "                   ('ohe', OneHotEncoder(handle_unknown='ignore'))]),\n",
       "   ['Grade _x',\n",
       "    'Reading \\nMet Winter Goal?',\n",
       "    'Math Met Winter Goal?',\n",
       "    'Race/Ethnicity',\n",
       "    'Language Code',\n",
       "    'English Language Proficiency Level',\n",
       "    'Migrant Status',\n",
       "    'Primary Disability Type']),\n",
       "  ('numeric',\n",
       "   Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                   ('imputer', SimpleImputer(strategy='median'))]),\n",
       "   ['Reading Fall Percentile',\n",
       "    'Reading Winter Percentile',\n",
       "    'Reading Fall Score to Winter Growth',\n",
       "    'Math Fall Percentile',\n",
       "    'Math Winter Percentile',\n",
       "    'Math Fall to Winter Growth',\n",
       "    'Math Met Winter Goal?',\n",
       "    'First Entry Date Into US School',\n",
       "    'LEP Entry Date',\n",
       "    'LEP Exit Date'])],\n",
       " 'preprocessing__verbose': False,\n",
       " 'preprocessing__boolean': Pipeline(steps=[('imputer',\n",
       "                  SimpleImputer(fill_value='False', strategy='constant')),\n",
       "                 ('ohe', OneHotEncoder(handle_unknown='ignore'))]),\n",
       " 'preprocessing__categorical': Pipeline(steps=[('imputer',\n",
       "                  SimpleImputer(fill_value='missing', strategy='constant')),\n",
       "                 ('ohe', OneHotEncoder(handle_unknown='ignore'))]),\n",
       " 'preprocessing__numeric': Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                 ('imputer', SimpleImputer(strategy='median'))]),\n",
       " 'preprocessing__boolean__memory': None,\n",
       " 'preprocessing__boolean__steps': [('imputer',\n",
       "   SimpleImputer(fill_value='False', strategy='constant')),\n",
       "  ('ohe', OneHotEncoder(handle_unknown='ignore'))],\n",
       " 'preprocessing__boolean__verbose': False,\n",
       " 'preprocessing__boolean__imputer': SimpleImputer(fill_value='False', strategy='constant'),\n",
       " 'preprocessing__boolean__ohe': OneHotEncoder(handle_unknown='ignore'),\n",
       " 'preprocessing__boolean__imputer__add_indicator': False,\n",
       " 'preprocessing__boolean__imputer__copy': True,\n",
       " 'preprocessing__boolean__imputer__fill_value': 'False',\n",
       " 'preprocessing__boolean__imputer__missing_values': nan,\n",
       " 'preprocessing__boolean__imputer__strategy': 'constant',\n",
       " 'preprocessing__boolean__imputer__verbose': 0,\n",
       " 'preprocessing__boolean__ohe__categories': 'auto',\n",
       " 'preprocessing__boolean__ohe__drop': None,\n",
       " 'preprocessing__boolean__ohe__dtype': numpy.float64,\n",
       " 'preprocessing__boolean__ohe__handle_unknown': 'ignore',\n",
       " 'preprocessing__boolean__ohe__sparse': True,\n",
       " 'preprocessing__categorical__memory': None,\n",
       " 'preprocessing__categorical__steps': [('imputer',\n",
       "   SimpleImputer(fill_value='missing', strategy='constant')),\n",
       "  ('ohe', OneHotEncoder(handle_unknown='ignore'))],\n",
       " 'preprocessing__categorical__verbose': False,\n",
       " 'preprocessing__categorical__imputer': SimpleImputer(fill_value='missing', strategy='constant'),\n",
       " 'preprocessing__categorical__ohe': OneHotEncoder(handle_unknown='ignore'),\n",
       " 'preprocessing__categorical__imputer__add_indicator': False,\n",
       " 'preprocessing__categorical__imputer__copy': True,\n",
       " 'preprocessing__categorical__imputer__fill_value': 'missing',\n",
       " 'preprocessing__categorical__imputer__missing_values': nan,\n",
       " 'preprocessing__categorical__imputer__strategy': 'constant',\n",
       " 'preprocessing__categorical__imputer__verbose': 0,\n",
       " 'preprocessing__categorical__ohe__categories': 'auto',\n",
       " 'preprocessing__categorical__ohe__drop': None,\n",
       " 'preprocessing__categorical__ohe__dtype': numpy.float64,\n",
       " 'preprocessing__categorical__ohe__handle_unknown': 'ignore',\n",
       " 'preprocessing__categorical__ohe__sparse': True,\n",
       " 'preprocessing__numeric__memory': None,\n",
       " 'preprocessing__numeric__steps': [('scaler', StandardScaler()),\n",
       "  ('imputer', SimpleImputer(strategy='median'))],\n",
       " 'preprocessing__numeric__verbose': False,\n",
       " 'preprocessing__numeric__scaler': StandardScaler(),\n",
       " 'preprocessing__numeric__imputer': SimpleImputer(strategy='median'),\n",
       " 'preprocessing__numeric__scaler__copy': True,\n",
       " 'preprocessing__numeric__scaler__with_mean': True,\n",
       " 'preprocessing__numeric__scaler__with_std': True,\n",
       " 'preprocessing__numeric__imputer__add_indicator': False,\n",
       " 'preprocessing__numeric__imputer__copy': True,\n",
       " 'preprocessing__numeric__imputer__fill_value': None,\n",
       " 'preprocessing__numeric__imputer__missing_values': nan,\n",
       " 'preprocessing__numeric__imputer__strategy': 'median',\n",
       " 'preprocessing__numeric__imputer__verbose': 0,\n",
       " 'classifier__alpha': 0.001,\n",
       " 'classifier__average': False,\n",
       " 'classifier__class_weight': 'balanced',\n",
       " 'classifier__early_stopping': True,\n",
       " 'classifier__epsilon': 0.1,\n",
       " 'classifier__eta0': 0.0,\n",
       " 'classifier__fit_intercept': False,\n",
       " 'classifier__l1_ratio': 0.15,\n",
       " 'classifier__learning_rate': 'optimal',\n",
       " 'classifier__loss': 'hinge',\n",
       " 'classifier__max_iter': 1000,\n",
       " 'classifier__n_iter_no_change': 5,\n",
       " 'classifier__n_jobs': None,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__power_t': 0.5,\n",
       " 'classifier__random_state': None,\n",
       " 'classifier__shuffle': True,\n",
       " 'classifier__tol': 0.001,\n",
       " 'classifier__validation_fraction': 0.1,\n",
       " 'classifier__verbose': 0,\n",
       " 'classifier__warm_start': False}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Search for hyperparameters \n",
    "\n",
    "pipe = Pipeline([('preprocessing', preprocessing), \n",
    "                 ('classifier',  SGDClassifier())])\n",
    "\n",
    "search_space = {'classifier__alpha': [0.0001, 0.001, 0.01],\n",
    "                 'classifier__class_weight': [None, 'balanced'],\n",
    "                 'classifier__early_stopping': [True,False],\n",
    "                 'classifier__fit_intercept': [True, False],\n",
    "                 'classifier__l1_ratio': [0.05, 0.15, 0.25],\n",
    "                 'classifier__loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "                 'classifier__max_iter': [1000, 2000],\n",
    "                 'classifier__penalty': ['l2', 'l1', 'elasticnet']}\n",
    "\n",
    "clf_rand = RandomizedSearchCV(estimator=pipe, \n",
    "                            param_distributions=search_space, \n",
    "                            n_iter=5,\n",
    "                            cv=5,\n",
    "                            verbose=True,\n",
    "                            )\n",
    "clf_rand.fit(X_train, y_train)\n",
    "clf_rand.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include only those that are different from the default values \n",
    "\n",
    "sgd_best_params =  {'alpha': 0.01,\n",
    "                    'early_stopping': True,\n",
    "                    'fit_intercept': False,\n",
    "                    'l1_ratio': 0.25,\n",
    "                    'max_iter': 2000,\n",
    "                    'penalty': 'l1'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean SGD Precision: 0.851\n",
      "Mean SGD Accuracy: 0.83\n"
     ]
    }
   ],
   "source": [
    "# Fit model using KFold CV\n",
    "\n",
    "pipe = Pipeline([('preprocessing', preprocessing), \n",
    "                 ('classifier', SGDClassifier(**sgd_best_params))])\n",
    "\n",
    "scores = cross_validate(pipe,\n",
    "                        X_train,\n",
    "                        y_train, \n",
    "                        cv=kfold, \n",
    "                        scoring=scoring)\n",
    "\n",
    "sgd_precision_score = scores['test_precision'].mean()\n",
    "sgd_acc_score = scores['test_accuracy'].mean()\n",
    "print(\"Mean SGD Precision:\", round(sgd_precision_score,3))\n",
    "print(\"Mean SGD Accuracy:\", round(sgd_acc_score,3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of 3 Candidate Models \n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Extra Trees</td>\n",
       "      <td>0.856</td>\n",
       "      <td>0.851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SGD</td>\n",
       "      <td>0.851</td>\n",
       "      <td>0.830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Precision  Accuracy\n",
       "0  Logistic Regression      0.909     0.837\n",
       "1          Extra Trees      0.856     0.851\n",
       "2                  SGD      0.851     0.830"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\"Model\": [\"Logistic Regression\", \"Extra Trees\", \"SGD\"], \n",
    "              \"Precision\": [lr_precision_score,et_precision_score,sgd_precision_score], \n",
    "              \"Accuracy\": [lr_acc_score,et_acc_score,sgd_acc_score]})\\\n",
    "            .round(3)\\\n",
    "            .sort_values(['Precision'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Logistic Regression had the highest precision and the 2nd highest accuracy, I willl select this as my final model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine Importances for Logistic Regression Model\n",
    "----\n",
    "Looking at the importances will help me:\n",
    "   1. **Reduce the number of features in my model.** \n",
    "       - For logistic regression models, the simpler the model, the better because it will increase the generality of the model. I will use the results of the permutation importance to keep only the features that have positive non-zero importances. \n",
    "   2. **Gain a better understanding about which features are most important to the logistic regression model.** \n",
    "       - This will help me answer my second question about which features teachers should focus on that seem to be most important for predicting student performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and validation set from original training set\n",
    "\n",
    "X_train_imp, X_valid_imp, y_train_imp, y_valid_imp = train_test_split(X_train, y_train, random_state=46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('preprocessing', preprocessing), \n",
    "                 ('classifier',  LogisticRegression(**lr_best_params))])\n",
    "\n",
    "model = pipe.fit(X_train_imp, y_train_imp)\n",
    "\n",
    "r = permutation_importance(model, \n",
    "                           X_valid_imp, y_valid_imp,  \n",
    "                           n_repeats=10,\n",
    "                           random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Math Fall Percentile 0.12054794520547943\n",
      "LEP Status 0.020547945205479423\n",
      "English Language Proficiency Level 0.015068493150684892\n",
      "Reading Fall Percentile 0.013698630136986278\n",
      "Reading Fall Score to Winter Growth 0.009589041095890405\n",
      "LEP Exit Date 0.0068493150684931225\n",
      "Grade _x 0.004109589041095862\n",
      "Reading Met Winter Goal? 0.004109589041095862\n",
      "Reading Winter Percentile 0.0027397260273972377\n",
      "LEP Entry Date 0.0013698630136986245\n",
      "ID  0.0\n",
      "Reading Fall '18 RIT 0.0\n",
      "Reading Typical RIT Growth Points 0.0\n",
      "Reading Tiered RIT Growth Points 0.0\n",
      "Reading Winter '18 GOAL Score 0.0\n",
      "Reading Winter '18 RIT 0.0\n",
      "Reading Spring '19 GOAL Score 0.0\n",
      "Reading Spring '19 RIT 0.0\n",
      "Reading Spring '19 %ile 0.0\n",
      "Reading Fall to Spring RIT Growth 0.0\n",
      "Reading Met Spring Goal? 0.0\n",
      "Math Fall '18 RIT 0.0\n",
      "Math Typical RIT Growth Points 0.0\n",
      "Math Tiered RIT Growth Points 0.0\n",
      "Math Winter '18 GOAL Score 0.0\n",
      "Math Winter '18 RIT 0.0\n",
      "Math Spring '19 GOAL Score 0.0\n",
      "Math Spring '19 RIT 0.0\n",
      "Math Spring '19 %ile 0.0\n",
      "Math Fall to Spring RIT Growth 0.0\n",
      "Math Met Spring Goal?  0.0\n",
      "Grade _y 0.0\n",
      "IDEA Indicator 0.0\n",
      "Section 504 Status 0.0\n",
      "Migrant Status 0.0\n",
      "Primary Disability Type 0.0\n",
      "ELA/Literacy OppNumber 0.0\n",
      "ELA/Literacy Scale Score 0.0\n",
      "Standard Error for ELA/Literacy Scale Score 0.0\n",
      "Reading Claim Achievement Category 0.0\n",
      "Writing Claim Achievement Category 0.0\n",
      "Listening Claim Achievement Category 0.0\n",
      "Research/Inquiry Claim Achievement Category 0.0\n",
      "Mathematics OppNumber 0.0\n",
      "Mathematics Scale Score 0.0\n",
      "Standard Error for Mathematics Scale Score 0.0\n",
      "Concepts and Procedures Claim Achievement Category 0.0\n",
      "Problem Solving and Modeling & Data Analysis Claim Achievement Category 0.0\n",
      "Communicating Reasoning Claim Achievement Category 0.0\n",
      "Language Code -0.005479452054794565\n",
      "Math Winter Percentile -0.006849315068493178\n",
      "First Entry Date Into US School -0.006849315068493178\n",
      "Race/Ethnicity -0.008219178082191803\n",
      "Math Fall to Winter Growth -0.010958904109589085\n",
      "Economically Disadvantaged Status -0.019178082191780844\n",
      "Math Met Winter Goal? -0.02054794520547949\n"
     ]
    }
   ],
   "source": [
    "# Print results of importances \n",
    "\n",
    "features = X.columns\n",
    "importances = r.importances_mean\n",
    "\n",
    "feature_importances = []\n",
    "for x in zip(features,importances):\n",
    "    feature_importances.append((x[0], x[1]))\n",
    "\n",
    "for x in sorted(feature_importances, key= lambda x: x[1], reverse=True):\n",
    "    print(x[0], x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove some codependent features and KFold CV again\n",
    "- Math/Reading Fall Percentile and Winter Percentile are similar\n",
    "    - Winter had negative importance and Fall had positive importance, so this is a sign I should keep just one\n",
    "    - I will keep Winter since it shows the most recent information\n",
    "- I will remove all features that had 0 or negative importance. \n",
    "- There are several features with extremely small importances. To keep the model simple, I will just keep the top 5. \n",
    "- The model originally had 23 features and now only has 5. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Logistic Regression Precision After Removing Features: 0.939\n",
      "Mean Logistic Regression Accuracy After Removing Features: 0.872\n",
      "\n",
      "Precision went from 0.909 to 0.939, an improvement of 0.03\n",
      "Accuracy went from 0.876 to 0.872, an improvement of -0.004\n"
     ]
    }
   ],
   "source": [
    "# Columns in simplified model \n",
    "boolean_columns_2 = ['LEP Status']\n",
    "\n",
    "categorical_columns_2 = ['English Language Proficiency Level']\n",
    "\n",
    "numeric_columns_2 = ['Math Winter Percentile', 'Reading Winter Percentile', 'Reading Fall Score to Winter Growth']\n",
    "\n",
    "\n",
    "preprocessing_2 = ColumnTransformer([('boolean', boolean_pipe,  boolean_columns_2),\n",
    "                                     ('categorical', categorical_pipe, categorical_columns_2),\n",
    "                                     ('numeric',  numeric_pipe, numeric_columns_2)])  \n",
    "\n",
    "# Updated pipeline\n",
    "pipe_2 = Pipeline([('preprocessing', preprocessing_2), \n",
    "                   ('classifier',  LogisticRegression(**lr_best_params))])\n",
    "\n",
    "scores = cross_validate(pipe_2,\n",
    "                        X_train,\n",
    "                        y_train, \n",
    "                        cv=kfold, \n",
    "                        scoring=scoring)\n",
    "\n",
    "lr_precision_score_2 = scores['test_precision'].mean()\n",
    "lr_acc_score_2 = scores['test_accuracy'].mean()\n",
    "print(\"Mean Logistic Regression Precision After Removing Features:\", round(lr_precision_score_2,3))\n",
    "print(\"Mean Logistic Regression Accuracy After Removing Features:\", round(lr_acc_score_2,3))\n",
    "print(\"\")\n",
    "print(f\"Precision went from {round(lr_precision_score,3)} to {round(lr_precision_score_2,3)}, an improvement of\", round(lr_precision_score_2 -lr_precision_score, 3))\n",
    "print(f\"Accuracy went from {round(lr_acc_score,3)} to {round(lr_acc_score_2,3)}, an improvement of\", round(lr_acc_score_2 -lr_acc_score, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing features increased the precision, but accuracy decreased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Model Results \n",
    "----\n",
    "Now it is time to run the final pipeline on the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Features\n",
    "boolean_columns_final = ['LEP Status']\n",
    "\n",
    "categorical_columns_final = ['English Language Proficiency Level']\n",
    "\n",
    "numeric_columns_final = ['Math Winter Percentile', 'Reading Winter Percentile', 'Reading Fall Score to Winter Growth']\n",
    "\n",
    "\n",
    "# Final Preprocessing Pipeline \n",
    "boolean_pipe = Pipeline([('imputer', SimpleImputer(strategy='constant', fill_value='False')), \n",
    "                         ('ohe', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "categorical_pipe = Pipeline([('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "                             ('ohe', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "numeric_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                         ('imputer', SimpleImputer(strategy='median'))])\n",
    "\n",
    "preprocessing_final = ColumnTransformer([('boolean', boolean_pipe,  boolean_columns_final),\n",
    "                                   ('categorical', categorical_pipe, categorical_columns_final),\n",
    "                                   ('numeric',  numeric_pipe, numeric_columns_final)])  \n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "lr_best_params = {'C': 7.742636826811269,\n",
    "                  'class_weight': 'balanced',\n",
    "                  'fit_intercept': False,\n",
    "                  'max_iter': 500,\n",
    "                  'penalty': 'l1',\n",
    "                  'solver': 'liblinear'}\n",
    "\n",
    "# Final Pipeline\n",
    "pipe_final = Pipeline([('preprocessing', preprocessing_final), \n",
    "                       ('classifier',  LogisticRegression(**lr_best_params))])\n",
    "\n",
    "# Fit Pipeline on entire train \n",
    "pipe_final.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "preds = pipe_final.predict(X_test)\n",
    "\n",
    "# See test set precision, accuraacy and confusion matrix \n",
    "lr_test_precision_score = precision_score(y_test, preds)\n",
    "lr_test_acc_score = accuracy_score(y_test, preds)\n",
    "lr_test_confusion = confusion_matrix(y_test, preds)\n",
    "\n",
    "print(\"Logistic Regression Test Set Precision:\", round(lr_test_precision_score,3))\n",
    "print(\"Logistic Regression Test Set Accuracy:\", round(lr_test_acc_score,3))\n",
    "print(\"\")\n",
    "print(\"Test Set Confusion Matrix:\")\n",
    "print(lr_test_confusion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred: Fail</th>\n",
       "      <th>Pred: Pass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual: Fail</th>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual: Pass</th>\n",
       "      <td>7</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Pred: Fail  Pred: Pass\n",
       "Actual: Fail          26           5\n",
       "Actual: Pass           7          59"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create df to interpret confusion matrix \n",
    "labels = ['Fail', 'Pass']\n",
    "pd.DataFrame(lr_test_confusion, columns=[f'Pred: {label}' for label in labels],\n",
    "                  index=[f'Actual: {label}' for label in labels])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.84      0.81        31\n",
      "           1       0.92      0.89      0.91        66\n",
      "\n",
      "    accuracy                           0.88        97\n",
      "   macro avg       0.85      0.87      0.86        97\n",
      "weighted avg       0.88      0.88      0.88        97\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check classification report \n",
    "full_report = classification_report(y_test, preds)\n",
    "print(full_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting that the model has high precision for class 1 but low precision for class 0. Recall for both classes is about the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions\n",
    "----\n",
    "\n",
    "**The model performed similarly on the test set as the train set**\n",
    "<br>The model had a precision of 0.939 on the train/validation set and a precision of 0.922 on the test set. The precision decreased only slightly, meaning the model has good generality. \n",
    "\n",
    "**The model minimizes false positives**\n",
    "<br>My final logistic regression model does a good job of correctly predicting students who will pass the SBAC. The model was selected and tuned to minimize the chance of false positives. In the test set, only 5 students are predicted to pass when in reality they failed. This model can help teachers identify students that need extra support who would have otherwise gone unnoticed.\n",
    "\n",
    "**The model's accuracy beats the a priori probability**\n",
    "<br> The model's accuracy is 0.88, well above the a priori probability of 0.66. There is still room for improvement in the accuracy score.\n",
    "\n",
    "**The feature that was most important to the model's predictions was Math Winter Percentile**\n",
    "It is unsurprising that Math Winter Percentile (derived from the student's score on the MAP test) is the most important predictor for performance on SBAC. The magnitude of this importance was much larger than all other features. This score was 0.12 while the second most important feature was 0.02. Even though there was a drastic drop in importance after the first feature, I decided to keep the top 5 features in the final model, which included: Math Winter Percentile, LEP Status, English Language Proficiency Level, Reading Fall Score to Winter Growth, and Reading Winter Percentile. \n",
    "\n",
    "**Areas for improvement:**\n",
    "- There is a big gap in the model's precision for class 0 (fail) and class 1 (pass). I would like to improve the model's ability to minimize false negatives. \n",
    "- Though minimizing false positives was more important than minimizing false negatives, it is still important to note that the model predicts that 7 students would fail when in reality they passed. The model incorrectly suggests that those 7 students should be receiving extra interventions. This is an issue because teachers would unnecessarily be increasing the load of students in need of extra support, taking away some of the resources from the students who need it most.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Steps\n",
    "----\n",
    "1. I'd be interested in re-doing this problem as a multi-class classification problem. The target class originally had 4 classes (Below Standard, Near Standard, Standard Met, and Standard Exceeded). I would be most interested in seeing how accurately I could predict which students would be in the Near Standard class. This would help teachers zoom in even more on the borderline students who could benefit from a little extra intervention. \n",
    "2. I would also like to try running this model using data only from Q2 and Q3 students. These are the students for which the outcomes tend to be less clear. I'd be curious to get data from multiple schools or multiple school years to see how the model performs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
