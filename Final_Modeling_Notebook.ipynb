{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "# Table of Contents\n",
    "1. Research Question\n",
    "2. Evaluation Metrics\n",
    "3. Load Data\n",
    "4. Search for best algorithm\n",
    "5. Tune hyperparameters for 3 candidate models\n",
    "6. Compare 3 candidate models and select final model\n",
    "7. Run final model on test set\n",
    "8. Examine feature importances for the final model\n",
    "9. Conclusion and next steps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Research Question \n",
    "----\n",
    "Can we use student demographic information and test scores throughout the year to predict whether or not a student will pass the Math SBAC at the end of the year? Which indicators from the first semester are most important for predicting whether or not a student will pass the Math SBAC? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Metrics  \n",
    "----\n",
    "2 evaluation metrics are used in this notebook: \n",
    "- precision\n",
    "- accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from   sklearn.base            import BaseEstimator\n",
    "from   sklearn.compose         import *\n",
    "from   sklearn.ensemble        import RandomForestClassifier, ExtraTreesClassifier, IsolationForest, GradientBoostingClassifier\n",
    "from   sklearn.experimental    import enable_iterative_imputer\n",
    "from   sklearn.impute          import *\n",
    "from   sklearn.inspection      import permutation_importance\n",
    "from   sklearn.linear_model    import LogisticRegression, PassiveAggressiveClassifier, RidgeClassifier, SGDClassifier\n",
    "from   sklearn.metrics         import precision_score, classification_report, accuracy_score, confusion_matrix\n",
    "from   sklearn.model_selection import train_test_split\n",
    "from   sklearn.model_selection import RandomizedSearchCV\n",
    "from   sklearn.neighbors       import *\n",
    "from   sklearn.pipeline        import Pipeline\n",
    "from   sklearn.preprocessing   import *\n",
    "from   sklearn.svm             import SVC\n",
    "from   sklearn.tree            import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('https://github.com/amtan20/predictSBAC/raw/main/public_student_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y dataframes \n",
    "X = data.drop(columns=['Mathematics Achievement Level','ELA/Literacy Achievement Level'])\n",
    "y = data['Mathematics Achievement Level']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up column names \n",
    "\n",
    "X.rename(columns={\"Reading Fall '18 %ile\" : \"Reading Fall Percentile\", \n",
    "                  \"Reading Winter '18 %ile\" : \"Reading Winter Percentile\", \n",
    "                  \"Math Fall '18 %ile\" : \"Math Fall Percentile\", \n",
    "                  \"Winter '18 %ile\" : \"Math Winter Percentile\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target Engineering \n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change y into binary numeric target \n",
    "def create_binary_target(y):\n",
    "    return np.where(y=='Standard Not Met',0, np.where(y=='Standard Nearly Met', 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use FunctionTransformer to apply the function to the target \n",
    "transformer = FunctionTransformer(create_binary_target)\n",
    "y = transformer.transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Train and Test Sets \n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('X_train.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Pipeline \n",
    "----\n",
    "Impute missing values, one hot encode categorical features, standardize numeric features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_columns = ['IDEA Indicator', 'LEP Status', 'Economically Disadvantaged Status']\n",
    "\n",
    "categorical_columns = ['Grade _x', \n",
    "       'Reading \\nMet Winter Goal?', \n",
    "       'Math Met Winter Goal?', \n",
    "       'Race/Ethnicity', 'Language Code',\n",
    "       'English Language Proficiency Level', 'Migrant Status',\n",
    "       'Primary Disability Type']\n",
    "\n",
    "numeric_columns = ['Reading Fall Percentile',\n",
    "       'Reading Winter Percentile', 'Reading Fall Score to Winter Growth',\n",
    "       'Math Fall Percentile',\n",
    "       'Math Winter Percentile', 'Math Fall to Winter Growth',\n",
    "       'Math Met Winter Goal?', \n",
    "       'First Entry Date Into US School', 'LEP Entry Date', 'LEP Exit Date']\n",
    "\n",
    "\n",
    "boolean_pipe = Pipeline([('imputer', SimpleImputer(strategy='constant', fill_value='False')), \n",
    "                         ('ohe', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "categorical_pipe = Pipeline([('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "                             ('ohe', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "numeric_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                         ('imputer', SimpleImputer(strategy='median'))])\n",
    "\n",
    "preprocessing = ColumnTransformer([('boolean', boolean_pipe,  boolean_columns),\n",
    "                                   ('categorical', categorical_pipe, categorical_columns),\n",
    "                                   ('numeric',  numeric_pipe, numeric_columns)])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search for best algorithm using RandomizedSearchCV\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper class (you do not have to use it)\n",
    "class DummyEstimator(BaseEstimator):\n",
    "    \"Pass through class, methods are present but do nothing.\"\n",
    "    def fit(self): pass\n",
    "    def score(self): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessing',\n",
       "                 ColumnTransformer(transformers=[('boolean',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(fill_value='False',\n",
       "                                                                                 strategy='constant')),\n",
       "                                                                  ('ohe',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                  ['IDEA Indicator',\n",
       "                                                   'LEP Status',\n",
       "                                                   'Economically Disadvantaged '\n",
       "                                                   'Status']),\n",
       "                                                 ('categorical',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(fill_value='missing'...\n",
       "                                                  Pipeline(steps=[('scaler',\n",
       "                                                                   StandardScaler()),\n",
       "                                                                  ('imputer',\n",
       "                                                                   SimpleImputer(strategy='median'))]),\n",
       "                                                  ['Reading Fall Percentile',\n",
       "                                                   'Reading Winter Percentile',\n",
       "                                                   'Reading Fall Score to '\n",
       "                                                   'Winter Growth',\n",
       "                                                   'Math Fall Percentile',\n",
       "                                                   'Math Winter Percentile',\n",
       "                                                   'Math Fall to Winter Growth',\n",
       "                                                   'Math Met Winter Goal?',\n",
       "                                                   'First Entry Date Into US '\n",
       "                                                   'School',\n",
       "                                                   'LEP Entry Date',\n",
       "                                                   'LEP Exit Date'])])),\n",
       "                ('mod', ExtraTreesClassifier())])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([('preprocessing', preprocessing),\n",
    "                 ('mod', DummyEstimator())])\n",
    "\n",
    "models = [{'mod' : [ExtraTreesClassifier()]},\n",
    "          {'mod' : [GradientBoostingClassifier()]},\n",
    "          {'mod' : [KNeighborsClassifier()]},\n",
    "          {'mod' : [LogisticRegression()]},\n",
    "          {'mod' : [PassiveAggressiveClassifier()]}\n",
    "          {'mod' : [RandomForestClassifier()]},\n",
    "          {'mod' : [RidgeClassifier()]},\n",
    "          {'mod' : [SGDClassifier()]},\n",
    "          {'mod' : [SVC()]}\n",
    "          ]\n",
    "\n",
    "clf_rand = RandomizedSearchCV(estimator=pipe, \n",
    "                              param_distributions=models, \n",
    "                              n_iter=3,\n",
    "                              cv=5,\n",
    "                              scoring='precision',\n",
    "                              n_jobs=-1)\n",
    "best_model = clf_rand.fit(X_train, y_train) \n",
    "best_model.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual Search for 2 more candidate algorithms \n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = [ExtraTreesClassifier(), \n",
    "              GradientBoostingClassifier()\n",
    "              KNeighborsClassifier(),\n",
    "              LogisticRegression(), \n",
    "              PassiveAggressiveClassifier(), \n",
    "              RandomForestClassifier(),\n",
    "              RidgeClassifier(), \n",
    "              SGDClassifier(), \n",
    "              SVC()    \n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier() 0.871\n",
      "LogisticRegression() 0.868\n",
      "ExtraTreesClassifier() 0.864\n",
      "RidgeClassifier() 0.851\n",
      "GradientBoostingClassifier() 0.843\n",
      "SGDClassifier() 0.842\n",
      "PassiveAggressiveClassifier() 0.836\n",
      "DecisionTreeClassifier() 0.804\n"
     ]
    }
   ],
   "source": [
    "# Check accuracy 100 times \n",
    "\n",
    "final_acc_results = []\n",
    "\n",
    "for algo in algorithms: \n",
    "    pipe = Pipeline([('preprocessing', preprocessing), \n",
    "                     ('classifier',  algo)])\n",
    "    acc_results = []\n",
    "    for x in range(100): \n",
    "        \n",
    "        X_train1, X_valid, y_train1, y_valid = train_test_split(X_train, y_train)\n",
    "        \n",
    "        pipe.fit(X_train1, y_train1)\n",
    "\n",
    "        y_pred = pipe.predict(X_valid)\n",
    "        acc_test = accuracy_score(y_valid, y_pred)\n",
    "        acc_results.append(acc_test)\n",
    "        avg_acc = sum(acc_results)/len(acc_results)\n",
    "    \n",
    "    final_acc_results.append((algo, round(avg_acc,3)))\n",
    "\n",
    "final_acc_results = sorted(final_acc_results, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "for x in final_acc_results:\n",
    "    print(x[0], x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier() 0.893\n",
      "ExtraTreesClassifier() 0.893\n",
      "LogisticRegression() 0.889\n",
      "RandomForestClassifier() 0.887\n",
      "PassiveAggressiveClassifier() 0.886\n",
      "GradientBoostingClassifier() 0.886\n",
      "RidgeClassifier() 0.875\n",
      "DecisionTreeClassifier() 0.851\n"
     ]
    }
   ],
   "source": [
    "# Check precision 100 times \n",
    "\n",
    "final_precision_results = []\n",
    "\n",
    "for algo in algorithms: \n",
    "    pipe = Pipeline([('preprocessing', preprocessing), \n",
    "                     ('classifier',  algo)])\n",
    "\n",
    "    precision_results = []\n",
    "    for x in range(100):  \n",
    "        \n",
    "        X_train1, X_valid, y_train1, y_valid = train_test_split(X_train, y_train)\n",
    "        \n",
    "        pipe.fit(X_train1, y_train1)\n",
    "\n",
    "        y_pred = pipe.predict(X_valid)\n",
    "        precision_test = precision_score(y_valid, y_pred)\n",
    "        precision_results.append(precision_test)\n",
    "        avg_precision = sum(precision_results)/len(precision_results)\n",
    "\n",
    "    final_precision_results.append((algo, round(avg_precision,3)))\n",
    "\n",
    "final_precision_results = sorted(final_precision_results, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "for x in final_precision_results:\n",
    "    print(x[0], x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune hyperparameters for candidate model 1: Logistic Regression Classifier \n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('preprocessing',\n",
       "   ColumnTransformer(transformers=[('boolean',\n",
       "                                    Pipeline(steps=[('imputer',\n",
       "                                                     SimpleImputer(fill_value='False',\n",
       "                                                                   strategy='constant')),\n",
       "                                                    ('ohe',\n",
       "                                                     OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                    ['IDEA Indicator', 'LEP Status',\n",
       "                                     'Economically Disadvantaged Status']),\n",
       "                                   ('categorical',\n",
       "                                    Pipeline(steps=[('imputer',\n",
       "                                                     SimpleImputer(fill_value='missing',\n",
       "                                                                   strategy='constant')),\n",
       "                                                    ('ohe',\n",
       "                                                     OneHo...\n",
       "                                     'Primary Disability Type']),\n",
       "                                   ('numeric',\n",
       "                                    Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                                    ('imputer',\n",
       "                                                     SimpleImputer(strategy='median'))]),\n",
       "                                    ['Reading Fall Percentile',\n",
       "                                     'Reading Winter Percentile',\n",
       "                                     'Reading Fall Score to Winter Growth',\n",
       "                                     'Math Fall Percentile',\n",
       "                                     'Math Winter Percentile',\n",
       "                                     'Math Fall to Winter Growth',\n",
       "                                     'Math Met Winter Goal?',\n",
       "                                     'First Entry Date Into US School',\n",
       "                                     'LEP Entry Date', 'LEP Exit Date'])])),\n",
       "  ('classifier',\n",
       "   LogisticRegression(C=7.742636826811269, class_weight='balanced', dual=True,\n",
       "                      fit_intercept=False, max_iter=500, multi_class='ovr',\n",
       "                      solver='liblinear'))],\n",
       " 'verbose': False,\n",
       " 'preprocessing': ColumnTransformer(transformers=[('boolean',\n",
       "                                  Pipeline(steps=[('imputer',\n",
       "                                                   SimpleImputer(fill_value='False',\n",
       "                                                                 strategy='constant')),\n",
       "                                                  ('ohe',\n",
       "                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                  ['IDEA Indicator', 'LEP Status',\n",
       "                                   'Economically Disadvantaged Status']),\n",
       "                                 ('categorical',\n",
       "                                  Pipeline(steps=[('imputer',\n",
       "                                                   SimpleImputer(fill_value='missing',\n",
       "                                                                 strategy='constant')),\n",
       "                                                  ('ohe',\n",
       "                                                   OneHo...\n",
       "                                   'Primary Disability Type']),\n",
       "                                 ('numeric',\n",
       "                                  Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                                  ('imputer',\n",
       "                                                   SimpleImputer(strategy='median'))]),\n",
       "                                  ['Reading Fall Percentile',\n",
       "                                   'Reading Winter Percentile',\n",
       "                                   'Reading Fall Score to Winter Growth',\n",
       "                                   'Math Fall Percentile',\n",
       "                                   'Math Winter Percentile',\n",
       "                                   'Math Fall to Winter Growth',\n",
       "                                   'Math Met Winter Goal?',\n",
       "                                   'First Entry Date Into US School',\n",
       "                                   'LEP Entry Date', 'LEP Exit Date'])]),\n",
       " 'classifier': LogisticRegression(C=7.742636826811269, class_weight='balanced', dual=True,\n",
       "                    fit_intercept=False, max_iter=500, multi_class='ovr',\n",
       "                    solver='liblinear'),\n",
       " 'preprocessing__n_jobs': None,\n",
       " 'preprocessing__remainder': 'drop',\n",
       " 'preprocessing__sparse_threshold': 0.3,\n",
       " 'preprocessing__transformer_weights': None,\n",
       " 'preprocessing__transformers': [('boolean',\n",
       "   Pipeline(steps=[('imputer',\n",
       "                    SimpleImputer(fill_value='False', strategy='constant')),\n",
       "                   ('ohe', OneHotEncoder(handle_unknown='ignore'))]),\n",
       "   ['IDEA Indicator', 'LEP Status', 'Economically Disadvantaged Status']),\n",
       "  ('categorical',\n",
       "   Pipeline(steps=[('imputer',\n",
       "                    SimpleImputer(fill_value='missing', strategy='constant')),\n",
       "                   ('ohe', OneHotEncoder(handle_unknown='ignore'))]),\n",
       "   ['Grade _x',\n",
       "    'Reading \\nMet Winter Goal?',\n",
       "    'Math Met Winter Goal?',\n",
       "    'Race/Ethnicity',\n",
       "    'Language Code',\n",
       "    'English Language Proficiency Level',\n",
       "    'Migrant Status',\n",
       "    'Primary Disability Type']),\n",
       "  ('numeric',\n",
       "   Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                   ('imputer', SimpleImputer(strategy='median'))]),\n",
       "   ['Reading Fall Percentile',\n",
       "    'Reading Winter Percentile',\n",
       "    'Reading Fall Score to Winter Growth',\n",
       "    'Math Fall Percentile',\n",
       "    'Math Winter Percentile',\n",
       "    'Math Fall to Winter Growth',\n",
       "    'Math Met Winter Goal?',\n",
       "    'First Entry Date Into US School',\n",
       "    'LEP Entry Date',\n",
       "    'LEP Exit Date'])],\n",
       " 'preprocessing__verbose': False,\n",
       " 'preprocessing__boolean': Pipeline(steps=[('imputer',\n",
       "                  SimpleImputer(fill_value='False', strategy='constant')),\n",
       "                 ('ohe', OneHotEncoder(handle_unknown='ignore'))]),\n",
       " 'preprocessing__categorical': Pipeline(steps=[('imputer',\n",
       "                  SimpleImputer(fill_value='missing', strategy='constant')),\n",
       "                 ('ohe', OneHotEncoder(handle_unknown='ignore'))]),\n",
       " 'preprocessing__numeric': Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                 ('imputer', SimpleImputer(strategy='median'))]),\n",
       " 'preprocessing__boolean__memory': None,\n",
       " 'preprocessing__boolean__steps': [('imputer',\n",
       "   SimpleImputer(fill_value='False', strategy='constant')),\n",
       "  ('ohe', OneHotEncoder(handle_unknown='ignore'))],\n",
       " 'preprocessing__boolean__verbose': False,\n",
       " 'preprocessing__boolean__imputer': SimpleImputer(fill_value='False', strategy='constant'),\n",
       " 'preprocessing__boolean__ohe': OneHotEncoder(handle_unknown='ignore'),\n",
       " 'preprocessing__boolean__imputer__add_indicator': False,\n",
       " 'preprocessing__boolean__imputer__copy': True,\n",
       " 'preprocessing__boolean__imputer__fill_value': 'False',\n",
       " 'preprocessing__boolean__imputer__missing_values': nan,\n",
       " 'preprocessing__boolean__imputer__strategy': 'constant',\n",
       " 'preprocessing__boolean__imputer__verbose': 0,\n",
       " 'preprocessing__boolean__ohe__categories': 'auto',\n",
       " 'preprocessing__boolean__ohe__drop': None,\n",
       " 'preprocessing__boolean__ohe__dtype': numpy.float64,\n",
       " 'preprocessing__boolean__ohe__handle_unknown': 'ignore',\n",
       " 'preprocessing__boolean__ohe__sparse': True,\n",
       " 'preprocessing__categorical__memory': None,\n",
       " 'preprocessing__categorical__steps': [('imputer',\n",
       "   SimpleImputer(fill_value='missing', strategy='constant')),\n",
       "  ('ohe', OneHotEncoder(handle_unknown='ignore'))],\n",
       " 'preprocessing__categorical__verbose': False,\n",
       " 'preprocessing__categorical__imputer': SimpleImputer(fill_value='missing', strategy='constant'),\n",
       " 'preprocessing__categorical__ohe': OneHotEncoder(handle_unknown='ignore'),\n",
       " 'preprocessing__categorical__imputer__add_indicator': False,\n",
       " 'preprocessing__categorical__imputer__copy': True,\n",
       " 'preprocessing__categorical__imputer__fill_value': 'missing',\n",
       " 'preprocessing__categorical__imputer__missing_values': nan,\n",
       " 'preprocessing__categorical__imputer__strategy': 'constant',\n",
       " 'preprocessing__categorical__imputer__verbose': 0,\n",
       " 'preprocessing__categorical__ohe__categories': 'auto',\n",
       " 'preprocessing__categorical__ohe__drop': None,\n",
       " 'preprocessing__categorical__ohe__dtype': numpy.float64,\n",
       " 'preprocessing__categorical__ohe__handle_unknown': 'ignore',\n",
       " 'preprocessing__categorical__ohe__sparse': True,\n",
       " 'preprocessing__numeric__memory': None,\n",
       " 'preprocessing__numeric__steps': [('scaler', StandardScaler()),\n",
       "  ('imputer', SimpleImputer(strategy='median'))],\n",
       " 'preprocessing__numeric__verbose': False,\n",
       " 'preprocessing__numeric__scaler': StandardScaler(),\n",
       " 'preprocessing__numeric__imputer': SimpleImputer(strategy='median'),\n",
       " 'preprocessing__numeric__scaler__copy': True,\n",
       " 'preprocessing__numeric__scaler__with_mean': True,\n",
       " 'preprocessing__numeric__scaler__with_std': True,\n",
       " 'preprocessing__numeric__imputer__add_indicator': False,\n",
       " 'preprocessing__numeric__imputer__copy': True,\n",
       " 'preprocessing__numeric__imputer__fill_value': None,\n",
       " 'preprocessing__numeric__imputer__missing_values': nan,\n",
       " 'preprocessing__numeric__imputer__strategy': 'median',\n",
       " 'preprocessing__numeric__imputer__verbose': 0,\n",
       " 'classifier__C': 7.742636826811269,\n",
       " 'classifier__class_weight': 'balanced',\n",
       " 'classifier__dual': True,\n",
       " 'classifier__fit_intercept': False,\n",
       " 'classifier__intercept_scaling': 1,\n",
       " 'classifier__l1_ratio': None,\n",
       " 'classifier__max_iter': 500,\n",
       " 'classifier__multi_class': 'ovr',\n",
       " 'classifier__n_jobs': None,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__random_state': None,\n",
       " 'classifier__solver': 'liblinear',\n",
       " 'classifier__tol': 0.0001,\n",
       " 'classifier__verbose': 0,\n",
       " 'classifier__warm_start': False}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Search for hyperparameters \n",
    "\n",
    "pipe = Pipeline([('preprocessing', preprocessing), \n",
    "                 ('classifier',  LogisticRegression())])\n",
    "\n",
    "search_space = {'classifier__C': np.logspace(0, 4, 10),\n",
    "                'classifier__class_weight': [None,'balanced'],\n",
    "                'classifier__dual': [True,False],\n",
    "                'classifier__fit_intercept': [True,False],\n",
    "                'classifier__max_iter': [10, 100, 500],\n",
    "                'classifier__multi_class': ['auto', 'ovr', 'multinomial'],\n",
    "                'classifier__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "                'classifier__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n",
    "\n",
    "clf_rand = RandomizedSearchCV(estimator=pipe, \n",
    "                            param_distributions=search_space, \n",
    "                            n_iter=5,\n",
    "                            cv=5,\n",
    "                            verbose=True)\n",
    "\n",
    "clf_rand.fit(X_train, y_train)\n",
    "clf_rand.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include only non-default params \n",
    "lr_best_params = {'C': 7.742636826811269,\n",
    "                  'class_weight': 'balanced',\n",
    "                  'fit_intercept': False,\n",
    "                  'max_iter': 500,\n",
    "                  'penalty': 'l1',\n",
    "                  'solver': 'liblinear'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Precision: 0.9\n",
      "Logistic Regression Accuracy: 0.822\n",
      "\n",
      "Confusion Matrix:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[24,  4],\n",
       "       [ 9, 36]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model\n",
    "pipe = Pipeline([('preprocessing', preprocessing), \n",
    "                 ('classifier',  LogisticRegression(**lr_best_params))])\n",
    "\n",
    "pipe.fit(X_train1, y_train1)\n",
    "preds = pipe.predict(X_valid)\n",
    "lr_precision_score = precision_score(y_valid, preds)\n",
    "lr_acc_score = accuracy_score(y_valid, preds)\n",
    "lr_confusion = confusion_matrix(y_valid, preds)\n",
    "\n",
    "print(\"Logistic Regression Precision:\", round(lr_precision_score,3))\n",
    "print(\"Logistic Regression Accuracy:\", round(lr_acc_score,3))\n",
    "print(\"\")\n",
    "print(\"Confusion Matrix:\")\n",
    "lr_confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** There were 4 students who were incorrectly predicted to pass who in reality failed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Tune hyperparameters for candidate model 2 : Extra Trees Classifier \n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    4.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('preprocessing',\n",
       "   ColumnTransformer(transformers=[('boolean',\n",
       "                                    Pipeline(steps=[('imputer',\n",
       "                                                     SimpleImputer(fill_value='False',\n",
       "                                                                   strategy='constant')),\n",
       "                                                    ('ohe',\n",
       "                                                     OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                    ['IDEA Indicator', 'LEP Status',\n",
       "                                     'Economically Disadvantaged Status']),\n",
       "                                   ('categorical',\n",
       "                                    Pipeline(steps=[('imputer',\n",
       "                                                     SimpleImputer(fill_value='missing',\n",
       "                                                                   strategy='constant')),\n",
       "                                                    ('ohe',\n",
       "                                                     OneHo...\n",
       "                                     'Primary Disability Type']),\n",
       "                                   ('numeric',\n",
       "                                    Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                                    ('imputer',\n",
       "                                                     SimpleImputer(strategy='median'))]),\n",
       "                                    ['Reading Fall Percentile',\n",
       "                                     'Reading Winter Percentile',\n",
       "                                     'Reading Fall Score to Winter Growth',\n",
       "                                     'Math Fall Percentile',\n",
       "                                     'Math Winter Percentile',\n",
       "                                     'Math Fall to Winter Growth',\n",
       "                                     'Math Met Winter Goal?',\n",
       "                                     'First Entry Date Into US School',\n",
       "                                     'LEP Entry Date', 'LEP Exit Date'])])),\n",
       "  ('classifier',\n",
       "   ExtraTreesClassifier(bootstrap=True, ccp_alpha=0.01, max_features='log2',\n",
       "                        min_samples_leaf=2, min_samples_split=3))],\n",
       " 'verbose': False,\n",
       " 'preprocessing': ColumnTransformer(transformers=[('boolean',\n",
       "                                  Pipeline(steps=[('imputer',\n",
       "                                                   SimpleImputer(fill_value='False',\n",
       "                                                                 strategy='constant')),\n",
       "                                                  ('ohe',\n",
       "                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                  ['IDEA Indicator', 'LEP Status',\n",
       "                                   'Economically Disadvantaged Status']),\n",
       "                                 ('categorical',\n",
       "                                  Pipeline(steps=[('imputer',\n",
       "                                                   SimpleImputer(fill_value='missing',\n",
       "                                                                 strategy='constant')),\n",
       "                                                  ('ohe',\n",
       "                                                   OneHo...\n",
       "                                   'Primary Disability Type']),\n",
       "                                 ('numeric',\n",
       "                                  Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                                  ('imputer',\n",
       "                                                   SimpleImputer(strategy='median'))]),\n",
       "                                  ['Reading Fall Percentile',\n",
       "                                   'Reading Winter Percentile',\n",
       "                                   'Reading Fall Score to Winter Growth',\n",
       "                                   'Math Fall Percentile',\n",
       "                                   'Math Winter Percentile',\n",
       "                                   'Math Fall to Winter Growth',\n",
       "                                   'Math Met Winter Goal?',\n",
       "                                   'First Entry Date Into US School',\n",
       "                                   'LEP Entry Date', 'LEP Exit Date'])]),\n",
       " 'classifier': ExtraTreesClassifier(bootstrap=True, ccp_alpha=0.01, max_features='log2',\n",
       "                      min_samples_leaf=2, min_samples_split=3),\n",
       " 'preprocessing__n_jobs': None,\n",
       " 'preprocessing__remainder': 'drop',\n",
       " 'preprocessing__sparse_threshold': 0.3,\n",
       " 'preprocessing__transformer_weights': None,\n",
       " 'preprocessing__transformers': [('boolean',\n",
       "   Pipeline(steps=[('imputer',\n",
       "                    SimpleImputer(fill_value='False', strategy='constant')),\n",
       "                   ('ohe', OneHotEncoder(handle_unknown='ignore'))]),\n",
       "   ['IDEA Indicator', 'LEP Status', 'Economically Disadvantaged Status']),\n",
       "  ('categorical',\n",
       "   Pipeline(steps=[('imputer',\n",
       "                    SimpleImputer(fill_value='missing', strategy='constant')),\n",
       "                   ('ohe', OneHotEncoder(handle_unknown='ignore'))]),\n",
       "   ['Grade _x',\n",
       "    'Reading \\nMet Winter Goal?',\n",
       "    'Math Met Winter Goal?',\n",
       "    'Race/Ethnicity',\n",
       "    'Language Code',\n",
       "    'English Language Proficiency Level',\n",
       "    'Migrant Status',\n",
       "    'Primary Disability Type']),\n",
       "  ('numeric',\n",
       "   Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                   ('imputer', SimpleImputer(strategy='median'))]),\n",
       "   ['Reading Fall Percentile',\n",
       "    'Reading Winter Percentile',\n",
       "    'Reading Fall Score to Winter Growth',\n",
       "    'Math Fall Percentile',\n",
       "    'Math Winter Percentile',\n",
       "    'Math Fall to Winter Growth',\n",
       "    'Math Met Winter Goal?',\n",
       "    'First Entry Date Into US School',\n",
       "    'LEP Entry Date',\n",
       "    'LEP Exit Date'])],\n",
       " 'preprocessing__verbose': False,\n",
       " 'preprocessing__boolean': Pipeline(steps=[('imputer',\n",
       "                  SimpleImputer(fill_value='False', strategy='constant')),\n",
       "                 ('ohe', OneHotEncoder(handle_unknown='ignore'))]),\n",
       " 'preprocessing__categorical': Pipeline(steps=[('imputer',\n",
       "                  SimpleImputer(fill_value='missing', strategy='constant')),\n",
       "                 ('ohe', OneHotEncoder(handle_unknown='ignore'))]),\n",
       " 'preprocessing__numeric': Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                 ('imputer', SimpleImputer(strategy='median'))]),\n",
       " 'preprocessing__boolean__memory': None,\n",
       " 'preprocessing__boolean__steps': [('imputer',\n",
       "   SimpleImputer(fill_value='False', strategy='constant')),\n",
       "  ('ohe', OneHotEncoder(handle_unknown='ignore'))],\n",
       " 'preprocessing__boolean__verbose': False,\n",
       " 'preprocessing__boolean__imputer': SimpleImputer(fill_value='False', strategy='constant'),\n",
       " 'preprocessing__boolean__ohe': OneHotEncoder(handle_unknown='ignore'),\n",
       " 'preprocessing__boolean__imputer__add_indicator': False,\n",
       " 'preprocessing__boolean__imputer__copy': True,\n",
       " 'preprocessing__boolean__imputer__fill_value': 'False',\n",
       " 'preprocessing__boolean__imputer__missing_values': nan,\n",
       " 'preprocessing__boolean__imputer__strategy': 'constant',\n",
       " 'preprocessing__boolean__imputer__verbose': 0,\n",
       " 'preprocessing__boolean__ohe__categories': 'auto',\n",
       " 'preprocessing__boolean__ohe__drop': None,\n",
       " 'preprocessing__boolean__ohe__dtype': numpy.float64,\n",
       " 'preprocessing__boolean__ohe__handle_unknown': 'ignore',\n",
       " 'preprocessing__boolean__ohe__sparse': True,\n",
       " 'preprocessing__categorical__memory': None,\n",
       " 'preprocessing__categorical__steps': [('imputer',\n",
       "   SimpleImputer(fill_value='missing', strategy='constant')),\n",
       "  ('ohe', OneHotEncoder(handle_unknown='ignore'))],\n",
       " 'preprocessing__categorical__verbose': False,\n",
       " 'preprocessing__categorical__imputer': SimpleImputer(fill_value='missing', strategy='constant'),\n",
       " 'preprocessing__categorical__ohe': OneHotEncoder(handle_unknown='ignore'),\n",
       " 'preprocessing__categorical__imputer__add_indicator': False,\n",
       " 'preprocessing__categorical__imputer__copy': True,\n",
       " 'preprocessing__categorical__imputer__fill_value': 'missing',\n",
       " 'preprocessing__categorical__imputer__missing_values': nan,\n",
       " 'preprocessing__categorical__imputer__strategy': 'constant',\n",
       " 'preprocessing__categorical__imputer__verbose': 0,\n",
       " 'preprocessing__categorical__ohe__categories': 'auto',\n",
       " 'preprocessing__categorical__ohe__drop': None,\n",
       " 'preprocessing__categorical__ohe__dtype': numpy.float64,\n",
       " 'preprocessing__categorical__ohe__handle_unknown': 'ignore',\n",
       " 'preprocessing__categorical__ohe__sparse': True,\n",
       " 'preprocessing__numeric__memory': None,\n",
       " 'preprocessing__numeric__steps': [('scaler', StandardScaler()),\n",
       "  ('imputer', SimpleImputer(strategy='median'))],\n",
       " 'preprocessing__numeric__verbose': False,\n",
       " 'preprocessing__numeric__scaler': StandardScaler(),\n",
       " 'preprocessing__numeric__imputer': SimpleImputer(strategy='median'),\n",
       " 'preprocessing__numeric__scaler__copy': True,\n",
       " 'preprocessing__numeric__scaler__with_mean': True,\n",
       " 'preprocessing__numeric__scaler__with_std': True,\n",
       " 'preprocessing__numeric__imputer__add_indicator': False,\n",
       " 'preprocessing__numeric__imputer__copy': True,\n",
       " 'preprocessing__numeric__imputer__fill_value': None,\n",
       " 'preprocessing__numeric__imputer__missing_values': nan,\n",
       " 'preprocessing__numeric__imputer__strategy': 'median',\n",
       " 'preprocessing__numeric__imputer__verbose': 0,\n",
       " 'classifier__bootstrap': True,\n",
       " 'classifier__ccp_alpha': 0.01,\n",
       " 'classifier__class_weight': None,\n",
       " 'classifier__criterion': 'gini',\n",
       " 'classifier__max_depth': None,\n",
       " 'classifier__max_features': 'log2',\n",
       " 'classifier__max_leaf_nodes': None,\n",
       " 'classifier__max_samples': None,\n",
       " 'classifier__min_impurity_decrease': 0.0,\n",
       " 'classifier__min_impurity_split': None,\n",
       " 'classifier__min_samples_leaf': 2,\n",
       " 'classifier__min_samples_split': 3,\n",
       " 'classifier__min_weight_fraction_leaf': 0.0,\n",
       " 'classifier__n_estimators': 100,\n",
       " 'classifier__n_jobs': None,\n",
       " 'classifier__oob_score': False,\n",
       " 'classifier__random_state': None,\n",
       " 'classifier__verbose': 0,\n",
       " 'classifier__warm_start': False}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Search for hyperparameters \n",
    "\n",
    "pipe = Pipeline([('preprocessing', preprocessing), \n",
    "                 ('classifier',  ExtraTreesClassifier())])\n",
    "\n",
    "search_space = {'classifier__bootstrap': [False,True],\n",
    "                'classifier__ccp_alpha': [0, 0.1, 0.01],\n",
    "                'classifier__class_weight': [None, 'balanced', 'balanced_subsample'],\n",
    "                'classifier__criterion': ['gini', 'entropy'],\n",
    "                'classifier__max_features': ['auto','sqrt', 'log2'],\n",
    "                'classifier__max_samples': [0.25,0.5, 0.75, None],\n",
    "                'classifier__min_samples_leaf': [1,2,3],\n",
    "                'classifier__min_samples_split': [2,3],\n",
    "                'classifier__min_weight_fraction_leaf': [0.0, 0.1],\n",
    "                'classifier__n_estimators': [10,100,500]}\n",
    "\n",
    "clf_rand = RandomizedSearchCV(estimator=pipe, \n",
    "                            param_distributions=search_space, \n",
    "                            n_iter=5,\n",
    "                            cv=5,\n",
    "                            verbose=True,\n",
    "                            )\n",
    "clf_rand.fit(X_train, y_train)\n",
    "clf_rand.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only include non-default params \n",
    "\n",
    "et_best_params = {'bootstrap': True,\n",
    "                  'min_samples_leaf': 2,\n",
    "                  'min_samples_split': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Trees Precision: 0.86\n",
      "Extra Trees Accuracy: 0.877\n",
      "\n",
      "Confusion Matrix:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[21,  7],\n",
       "       [ 2, 43]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model\n",
    "\n",
    "pipe = Pipeline([('preprocessing', preprocessing), \n",
    "                 ('classifier',  ExtraTreesClassifier(**et_best_params))])\n",
    "\n",
    "pipe.fit(X_train1, y_train1)\n",
    "preds = pipe.predict(X_valid)\n",
    "et_precision_score = precision_score(y_valid, preds)\n",
    "et_acc_score = accuracy_score(y_valid, preds)\n",
    "et_confusion = confusion_matrix(y_valid, preds)\n",
    "\n",
    "print(\"Extra Trees Precision:\", round(et_precision_score,3))\n",
    "print(\"Extra Trees Accuracy:\", round(et_acc_score,3))\n",
    "print(\"\")\n",
    "print(\"Confusion Matrix:\")\n",
    "et_confusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This model only misidentifies 7 students as passing when in reality they fail. This did not perform as well as the logistic regression model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune hyperparameters for candidate model 3: SGD Classifier \n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('preprocessing',\n",
       "   ColumnTransformer(transformers=[('boolean',\n",
       "                                    Pipeline(steps=[('imputer',\n",
       "                                                     SimpleImputer(fill_value='False',\n",
       "                                                                   strategy='constant')),\n",
       "                                                    ('ohe',\n",
       "                                                     OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                    ['IDEA Indicator', 'LEP Status',\n",
       "                                     'Economically Disadvantaged Status']),\n",
       "                                   ('categorical',\n",
       "                                    Pipeline(steps=[('imputer',\n",
       "                                                     SimpleImputer(fill_value='missing',\n",
       "                                                                   strategy='constant')),\n",
       "                                                    ('ohe',\n",
       "                                                     OneHo...\n",
       "                                     'Primary Disability Type']),\n",
       "                                   ('numeric',\n",
       "                                    Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                                    ('imputer',\n",
       "                                                     SimpleImputer(strategy='median'))]),\n",
       "                                    ['Reading Fall Percentile',\n",
       "                                     'Reading Winter Percentile',\n",
       "                                     'Reading Fall Score to Winter Growth',\n",
       "                                     'Math Fall Percentile',\n",
       "                                     'Math Winter Percentile',\n",
       "                                     'Math Fall to Winter Growth',\n",
       "                                     'Math Met Winter Goal?',\n",
       "                                     'First Entry Date Into US School',\n",
       "                                     'LEP Entry Date', 'LEP Exit Date'])])),\n",
       "  ('classifier',\n",
       "   SGDClassifier(alpha=0.01, early_stopping=True, l1_ratio=0.05, max_iter=2000))],\n",
       " 'verbose': False,\n",
       " 'preprocessing': ColumnTransformer(transformers=[('boolean',\n",
       "                                  Pipeline(steps=[('imputer',\n",
       "                                                   SimpleImputer(fill_value='False',\n",
       "                                                                 strategy='constant')),\n",
       "                                                  ('ohe',\n",
       "                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                  ['IDEA Indicator', 'LEP Status',\n",
       "                                   'Economically Disadvantaged Status']),\n",
       "                                 ('categorical',\n",
       "                                  Pipeline(steps=[('imputer',\n",
       "                                                   SimpleImputer(fill_value='missing',\n",
       "                                                                 strategy='constant')),\n",
       "                                                  ('ohe',\n",
       "                                                   OneHo...\n",
       "                                   'Primary Disability Type']),\n",
       "                                 ('numeric',\n",
       "                                  Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                                  ('imputer',\n",
       "                                                   SimpleImputer(strategy='median'))]),\n",
       "                                  ['Reading Fall Percentile',\n",
       "                                   'Reading Winter Percentile',\n",
       "                                   'Reading Fall Score to Winter Growth',\n",
       "                                   'Math Fall Percentile',\n",
       "                                   'Math Winter Percentile',\n",
       "                                   'Math Fall to Winter Growth',\n",
       "                                   'Math Met Winter Goal?',\n",
       "                                   'First Entry Date Into US School',\n",
       "                                   'LEP Entry Date', 'LEP Exit Date'])]),\n",
       " 'classifier': SGDClassifier(alpha=0.01, early_stopping=True, l1_ratio=0.05, max_iter=2000),\n",
       " 'preprocessing__n_jobs': None,\n",
       " 'preprocessing__remainder': 'drop',\n",
       " 'preprocessing__sparse_threshold': 0.3,\n",
       " 'preprocessing__transformer_weights': None,\n",
       " 'preprocessing__transformers': [('boolean',\n",
       "   Pipeline(steps=[('imputer',\n",
       "                    SimpleImputer(fill_value='False', strategy='constant')),\n",
       "                   ('ohe', OneHotEncoder(handle_unknown='ignore'))]),\n",
       "   ['IDEA Indicator', 'LEP Status', 'Economically Disadvantaged Status']),\n",
       "  ('categorical',\n",
       "   Pipeline(steps=[('imputer',\n",
       "                    SimpleImputer(fill_value='missing', strategy='constant')),\n",
       "                   ('ohe', OneHotEncoder(handle_unknown='ignore'))]),\n",
       "   ['Grade _x',\n",
       "    'Reading \\nMet Winter Goal?',\n",
       "    'Math Met Winter Goal?',\n",
       "    'Race/Ethnicity',\n",
       "    'Language Code',\n",
       "    'English Language Proficiency Level',\n",
       "    'Migrant Status',\n",
       "    'Primary Disability Type']),\n",
       "  ('numeric',\n",
       "   Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                   ('imputer', SimpleImputer(strategy='median'))]),\n",
       "   ['Reading Fall Percentile',\n",
       "    'Reading Winter Percentile',\n",
       "    'Reading Fall Score to Winter Growth',\n",
       "    'Math Fall Percentile',\n",
       "    'Math Winter Percentile',\n",
       "    'Math Fall to Winter Growth',\n",
       "    'Math Met Winter Goal?',\n",
       "    'First Entry Date Into US School',\n",
       "    'LEP Entry Date',\n",
       "    'LEP Exit Date'])],\n",
       " 'preprocessing__verbose': False,\n",
       " 'preprocessing__boolean': Pipeline(steps=[('imputer',\n",
       "                  SimpleImputer(fill_value='False', strategy='constant')),\n",
       "                 ('ohe', OneHotEncoder(handle_unknown='ignore'))]),\n",
       " 'preprocessing__categorical': Pipeline(steps=[('imputer',\n",
       "                  SimpleImputer(fill_value='missing', strategy='constant')),\n",
       "                 ('ohe', OneHotEncoder(handle_unknown='ignore'))]),\n",
       " 'preprocessing__numeric': Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                 ('imputer', SimpleImputer(strategy='median'))]),\n",
       " 'preprocessing__boolean__memory': None,\n",
       " 'preprocessing__boolean__steps': [('imputer',\n",
       "   SimpleImputer(fill_value='False', strategy='constant')),\n",
       "  ('ohe', OneHotEncoder(handle_unknown='ignore'))],\n",
       " 'preprocessing__boolean__verbose': False,\n",
       " 'preprocessing__boolean__imputer': SimpleImputer(fill_value='False', strategy='constant'),\n",
       " 'preprocessing__boolean__ohe': OneHotEncoder(handle_unknown='ignore'),\n",
       " 'preprocessing__boolean__imputer__add_indicator': False,\n",
       " 'preprocessing__boolean__imputer__copy': True,\n",
       " 'preprocessing__boolean__imputer__fill_value': 'False',\n",
       " 'preprocessing__boolean__imputer__missing_values': nan,\n",
       " 'preprocessing__boolean__imputer__strategy': 'constant',\n",
       " 'preprocessing__boolean__imputer__verbose': 0,\n",
       " 'preprocessing__boolean__ohe__categories': 'auto',\n",
       " 'preprocessing__boolean__ohe__drop': None,\n",
       " 'preprocessing__boolean__ohe__dtype': numpy.float64,\n",
       " 'preprocessing__boolean__ohe__handle_unknown': 'ignore',\n",
       " 'preprocessing__boolean__ohe__sparse': True,\n",
       " 'preprocessing__categorical__memory': None,\n",
       " 'preprocessing__categorical__steps': [('imputer',\n",
       "   SimpleImputer(fill_value='missing', strategy='constant')),\n",
       "  ('ohe', OneHotEncoder(handle_unknown='ignore'))],\n",
       " 'preprocessing__categorical__verbose': False,\n",
       " 'preprocessing__categorical__imputer': SimpleImputer(fill_value='missing', strategy='constant'),\n",
       " 'preprocessing__categorical__ohe': OneHotEncoder(handle_unknown='ignore'),\n",
       " 'preprocessing__categorical__imputer__add_indicator': False,\n",
       " 'preprocessing__categorical__imputer__copy': True,\n",
       " 'preprocessing__categorical__imputer__fill_value': 'missing',\n",
       " 'preprocessing__categorical__imputer__missing_values': nan,\n",
       " 'preprocessing__categorical__imputer__strategy': 'constant',\n",
       " 'preprocessing__categorical__imputer__verbose': 0,\n",
       " 'preprocessing__categorical__ohe__categories': 'auto',\n",
       " 'preprocessing__categorical__ohe__drop': None,\n",
       " 'preprocessing__categorical__ohe__dtype': numpy.float64,\n",
       " 'preprocessing__categorical__ohe__handle_unknown': 'ignore',\n",
       " 'preprocessing__categorical__ohe__sparse': True,\n",
       " 'preprocessing__numeric__memory': None,\n",
       " 'preprocessing__numeric__steps': [('scaler', StandardScaler()),\n",
       "  ('imputer', SimpleImputer(strategy='median'))],\n",
       " 'preprocessing__numeric__verbose': False,\n",
       " 'preprocessing__numeric__scaler': StandardScaler(),\n",
       " 'preprocessing__numeric__imputer': SimpleImputer(strategy='median'),\n",
       " 'preprocessing__numeric__scaler__copy': True,\n",
       " 'preprocessing__numeric__scaler__with_mean': True,\n",
       " 'preprocessing__numeric__scaler__with_std': True,\n",
       " 'preprocessing__numeric__imputer__add_indicator': False,\n",
       " 'preprocessing__numeric__imputer__copy': True,\n",
       " 'preprocessing__numeric__imputer__fill_value': None,\n",
       " 'preprocessing__numeric__imputer__missing_values': nan,\n",
       " 'preprocessing__numeric__imputer__strategy': 'median',\n",
       " 'preprocessing__numeric__imputer__verbose': 0,\n",
       " 'classifier__alpha': 0.01,\n",
       " 'classifier__average': False,\n",
       " 'classifier__class_weight': None,\n",
       " 'classifier__early_stopping': True,\n",
       " 'classifier__epsilon': 0.1,\n",
       " 'classifier__eta0': 0.0,\n",
       " 'classifier__fit_intercept': True,\n",
       " 'classifier__l1_ratio': 0.05,\n",
       " 'classifier__learning_rate': 'optimal',\n",
       " 'classifier__loss': 'hinge',\n",
       " 'classifier__max_iter': 2000,\n",
       " 'classifier__n_iter_no_change': 5,\n",
       " 'classifier__n_jobs': None,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__power_t': 0.5,\n",
       " 'classifier__random_state': None,\n",
       " 'classifier__shuffle': True,\n",
       " 'classifier__tol': 0.001,\n",
       " 'classifier__validation_fraction': 0.1,\n",
       " 'classifier__verbose': 0,\n",
       " 'classifier__warm_start': False}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Search for hyperparameters \n",
    "\n",
    "pipe = Pipeline([('preprocessing', preprocessing), \n",
    "                 ('classifier',  SGDClassifier())])\n",
    "\n",
    "search_space = {'classifier__alpha': [0.0001, 0.001, 0.01],\n",
    "                 'classifier__class_weight': [None, 'balanced'],\n",
    "                 'classifier__early_stopping': [True,False],\n",
    "                 'classifier__fit_intercept': [True, False],\n",
    "                 'classifier__l1_ratio': [0.05, 0.15, 0.25],\n",
    "                 'classifier__loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "                 'classifier__max_iter': [1000, 2000],\n",
    "                 'classifier__penalty': ['l2', 'l1', 'elasticnet']}\n",
    "\n",
    "clf_rand = RandomizedSearchCV(estimator=pipe, \n",
    "                            param_distributions=search_space, \n",
    "                            n_iter=5,\n",
    "                            cv=5,\n",
    "                            verbose=True,\n",
    "                            )\n",
    "clf_rand.fit(X_train, y_train)\n",
    "clf_rand.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include only those that are different from the default values \n",
    "\n",
    "sgd_best_params =  {'alpha': 0.01,\n",
    "                    'early_stopping': True,\n",
    "                    'fit_intercept': False,\n",
    "                    'l1_ratio': 0.25,\n",
    "                    'max_iter': 2000,\n",
    "                    'penalty': 'l1'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Precision: 0.875\n",
      "SGD Accuracy: 0.877\n",
      "\n",
      "Confusion Matrix:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[22,  6],\n",
       "       [ 3, 42]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model \n",
    "\n",
    "pipe = Pipeline([('preprocessing', preprocessing), \n",
    "                 ('classifier',  SGDClassifier(**sgd_best_params))])\n",
    "\n",
    "pipe.fit(X_train1, y_train1)\n",
    "preds = pipe.predict(X_valid)\n",
    "sgd_precision_score = precision_score(y_valid, preds)\n",
    "sgd_acc_score = accuracy_score(y_valid, preds)\n",
    "sgd_confusion = confusion_matrix(y_valid, preds)\n",
    "\n",
    "print(\"SGD Precision:\", round(sgd_precision_score,3))\n",
    "print(\"SGD Accuracy:\", round(sgd_acc_score,3))\n",
    "print(\"\")\n",
    "print(\"Confusion Matrix:\")\n",
    "sgd_confusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of 3 Candidate Models \n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SGD</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Extra Trees</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Precision  Accuracy\n",
       "0  Logistic Regression      0.900     0.822\n",
       "2                  SGD      0.875     0.877\n",
       "1          Extra Trees      0.860     0.877"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\"Model\": [\"Logistic Regression\", \"Extra Trees\", \"SGD\"], \n",
    "              \"Precision\": [lr_precision_score,et_precision_score,sgd_precision_score], \n",
    "              \"Accuracy\": [lr_acc_score,et_acc_score,sgd_acc_score]})\\\n",
    "            .round(3)\\\n",
    "            .sort_values(['Precision'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Model Results \n",
    "----\n",
    "After tuning the hyperparameters, Logistic Regression had the highest precision but the lowest accuracy. SGD and Extra Trees were about the same. Since I care most about avoiding false positives, I am selecting Logistic Regression as my final model, though I am concerned about the lower accuracy rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Set Precision: 0.957\n",
      "Logistic Regression Set Accuracy: 0.897\n",
      "\n",
      "Test Set Confusion Matrix:\n",
      "[[21  3]\n",
      " [ 7 66]]\n"
     ]
    }
   ],
   "source": [
    "# Pipeline\n",
    "boolean_columns = ['IDEA Indicator', 'LEP Status', 'Economically Disadvantaged Status']\n",
    "\n",
    "categorical_columns = ['Grade _x', \n",
    "       'Reading \\nMet Winter Goal?', \n",
    "       'Math Met Winter Goal?', \n",
    "       'Race/Ethnicity', 'Language Code',\n",
    "       'English Language Proficiency Level', 'Migrant Status',\n",
    "       'Primary Disability Type']\n",
    "\n",
    "numeric_columns = ['Reading Fall Percentile',\n",
    "       'Reading Winter Percentile', 'Reading Fall Score to Winter Growth',\n",
    "       'Math Fall Percentile',\n",
    "       'Math Winter Percentile', 'Math Fall to Winter Growth',\n",
    "       'Math Met Winter Goal?', \n",
    "       'First Entry Date Into US School', 'LEP Entry Date', 'LEP Exit Date']\n",
    "\n",
    "\n",
    "boolean_pipe = Pipeline([('imputer', SimpleImputer(strategy='constant', fill_value='False')), \n",
    "                         ('ohe', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "categorical_pipe = Pipeline([('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "                             ('ohe', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "numeric_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                         ('imputer', SimpleImputer(strategy='median'))])\n",
    "\n",
    "preprocessing = ColumnTransformer([('boolean', boolean_pipe,  boolean_columns),\n",
    "                                   ('categorical', categorical_pipe, categorical_columns),\n",
    "                                   ('numeric',  numeric_pipe, numeric_columns)])  \n",
    "\n",
    "# Hyperparameters\n",
    "et_best_params = {'bootstrap': True,\n",
    "                  'min_samples_leaf': 2,\n",
    "                  'min_samples_split': 3}\n",
    "\n",
    "# Final Pipeline\n",
    "pipe = Pipeline([('preprocessing', preprocessing), \n",
    "                 ('classifier',  LogisticRegression(**lr_best_params))])\n",
    "\n",
    "# Fit Pipeline on entire train \n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "preds = pipe.predict(X_test)\n",
    "\n",
    "# See test set precision, accuraacy and confusion matrix \n",
    "lr_precision_score = precision_score(y_test, preds)\n",
    "lr_acc_score = accuracy_score(y_test, preds)\n",
    "lr_confusion = confusion_matrix(y_test, preds)\n",
    "\n",
    "print(\"Logistic Regression Set Precision:\", round(et_precision_score,3))\n",
    "print(\"Logistic Regression Set Accuracy:\", round(et_acc_score,3))\n",
    "print(\"\")\n",
    "print(\"Test Set Confusion Matrix:\")\n",
    "print(lr_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred: Fail</th>\n",
       "      <th>Pred: Pass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual: Fail</th>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual: Pass</th>\n",
       "      <td>7</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Pred: Fail  Pred: Pass\n",
       "Actual: Fail          21           3\n",
       "Actual: Pass           7          66"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create df to interpret confusion matrix \n",
    "labels = ['Fail', 'Pass']\n",
    "pd.DataFrame(et_confusion, columns=[f'Pred: {label}' for label in labels],\n",
    "                  index=[f'Actual: {label}' for label in labels])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.88      0.81        24\n",
      "           1       0.96      0.90      0.93        73\n",
      "\n",
      "    accuracy                           0.90        97\n",
      "   macro avg       0.85      0.89      0.87        97\n",
      "weighted avg       0.91      0.90      0.90        97\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check classification report \n",
    "full_report = classification_report(y_test, preds)\n",
    "print(full_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importances \n",
    "----\n",
    "Which features were most important in the Logistic Regression model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipe.fit(X_train, y_train)\n",
    "\n",
    "r = permutation_importance(model, \n",
    "                           X_test, y_test,  \n",
    "                           n_repeats=100,\n",
    "                           random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Math Winter Percentile 0.10824742268041239\n",
      "LEP Status 0.05103092783505158\n",
      "Reading Fall Percentile 0.040000000000000036\n",
      "Grade _x 0.03494845360824745\n",
      "Reading Fall Score to Winter Growth 0.03329896907216499\n",
      "English Language Proficiency Level 0.027525773195876325\n",
      "Race/Ethnicity 0.021237113402061882\n",
      "Economically Disadvantaged Status 0.0073195876288660155\n",
      "Primary Disability Type 0.0070103092783505485\n",
      "Language Code 0.004639175257731989\n",
      "Math Met Winter Goal? 0.003711340206185604\n",
      "IDEA Indicator 0.0005154639175257824\n",
      "LEP Entry Date 0.0003092783505154806\n",
      "ID  0.0\n",
      "Reading Fall '18 RIT 0.0\n",
      "Reading Typical RIT Growth Points 0.0\n",
      "Reading Tiered RIT Growth Points 0.0\n",
      "Reading Winter '18 GOAL Score 0.0\n",
      "Reading Winter '18 RIT 0.0\n",
      "Reading \n",
      "Met Winter Goal? 0.0\n",
      "Reading Spring '19 GOAL Score 0.0\n",
      "Reading Spring '19 RIT 0.0\n",
      "Reading Spring '19 %ile 0.0\n",
      "Reading Fall to Spring RIT Growth 0.0\n",
      "Reading Met Spring Goal? 0.0\n",
      "Math Fall '18 RIT 0.0\n",
      "Math Typical RIT Growth Points 0.0\n",
      "Math Tiered RIT Growth Points 0.0\n",
      "Math Winter '18 GOAL Score 0.0\n",
      "Math Winter '18 RIT 0.0\n",
      "Math Spring '19 GOAL Score 0.0\n",
      "Math Spring '19 RIT 0.0\n",
      "Math Spring '19 %ile 0.0\n",
      "Math Fall to Spring RIT Growth 0.0\n",
      "Math Met Spring Goal?  0.0\n",
      "Grade _y 0.0\n",
      "Section 504 Status 0.0\n",
      "Migrant Status 0.0\n",
      "First Entry Date Into US School 0.0\n",
      "LEP Exit Date 0.0\n",
      "ELA/Literacy OppNumber 0.0\n",
      "ELA/Literacy Scale Score 0.0\n",
      "Standard Error for ELA/Literacy Scale Score 0.0\n",
      "Reading Claim Achievement Category 0.0\n",
      "Writing Claim Achievement Category 0.0\n",
      "Listening Claim Achievement Category 0.0\n",
      "Research/Inquiry Claim Achievement Category 0.0\n",
      "Mathematics OppNumber 0.0\n",
      "Mathematics Scale Score 0.0\n",
      "Standard Error for Mathematics Scale Score 0.0\n",
      "Concepts and Procedures Claim Achievement Category 0.0\n",
      "Problem Solving and Modeling & Data Analysis Claim Achievement Category 0.0\n",
      "Communicating Reasoning Claim Achievement Category 0.0\n",
      "Math Fall to Winter Growth -0.004845360824742235\n",
      "Math Fall Percentile -0.006288659793814397\n",
      "Reading Winter Percentile -0.006597938144329863\n"
     ]
    }
   ],
   "source": [
    "features = X.columns\n",
    "importances = r.importances_mean\n",
    "\n",
    "feature_importances = []\n",
    "for x in zip(features,importances):\n",
    "    feature_importances.append((x[0], x[1]))\n",
    "\n",
    "for x in sorted(feature_importances, key= lambda x: x[1], reverse=True):\n",
    "    print(x[0], x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarks about importances**\n",
    "- I was not surprised that Math Winter Percentile from the MAP test was the most important feature in this model. This supports what we have observed at the school and reinforces many of the MAP score-based practices that we have in place at the school. \n",
    "- It is unsurprising that Math Fall Percentile has negative importance since it is highly correlated to Math Winter Percentile. The same is observed for Reading Winter Percentile and Reading Fall Percentile. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions and Next Steps\n",
    "----\n",
    "\n",
    "My final logistic regression model does a good job of correctly predicting students who will pass the SBAC. The model was selected and tuned to minimize the chance of false positives. In the test set, only 3 of the 97 students are predicted to pass when in reality they failed. Using this model can help teachers identify students that need extra support who would have otherwise gone unnoticed.\n",
    "\n",
    "It is important to note that in the test set, 7 students were predicted to fail who in reality passed. The model incorrectly suggests that these students should be receiving extra interventions. This could be an issue because teachers have limited time and resources and these students would be unneceessarily taking away some of the resources from the students who need it most. The problem of false negatives is not as significant as the problem of false positives in this case, but it is important to be aware of it. \n",
    "\n",
    "Though the model has a 96% chance of correctly predicting if a student will pass, it has only a 75% chance of correctly predicting if a student will fail. When I performed this analyses, I recognized there was a class imbalance, but I did not think it was significant enough to address. Given the major differences in precision for the two classes, I would like to consider addressing this class imbalance issue in the future using SMOTE. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
